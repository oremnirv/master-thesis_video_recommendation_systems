{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import csv\n",
    "#import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "#import cStringIO\n",
    "#! sudo /mnt1/anaconda2/bin/pip install s3fs\n",
    "#! sudo pip install s3fs\n",
    "#import s3fs\n",
    "#pd.options.display.max_columns = None\n",
    "#pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting s3fs\n",
      "  Downloading s3fs-0.1.2.tar.gz\n",
      "Requirement already satisfied: boto3 in /usr/lib/python2.7/dist-packages (from s3fs)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/lib/python2.7/dist-packages (from boto3->s3fs)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/lib/python2.7/dist-packages (from boto3->s3fs)\n",
      "Requirement already satisfied: botocore<1.6.0,>=1.5.0 in /usr/lib/python2.7/dist-packages (from boto3->s3fs)\n",
      "Requirement already satisfied: futures<4.0.0,>=2.2.0; python_version == \"2.6\" or python_version == \"2.7\" in /usr/lib/python2.7/dist-packages (from s3transfer<0.2.0,>=0.1.10->boto3->s3fs)\n",
      "Requirement already satisfied: docutils>=0.10 in /usr/lib/python2.7/dist-packages (from botocore<1.6.0,>=1.5.0->boto3->s3fs)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<1.6.0,>=1.5.0->boto3->s3fs)\n",
      "  Downloading python_dateutil-2.6.1-py2.py3-none-any.whl (194kB)\n",
      "\u001b[K    100% |████████████████████████████████| 194kB 2.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python2.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.6.0,>=1.5.0->boto3->s3fs)\n",
      "Building wheels for collected packages: s3fs\n",
      "  Running setup.py bdist_wheel for s3fs ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ad/85/31/07e4a6c225e249b5e55116d9dcf94282c14e7f8fcc65969d22\n",
      "Successfully built s3fs\n",
      "Installing collected packages: s3fs, python-dateutil\n",
      "  Found existing installation: python-dateutil 1.5\n",
      "    Uninstalling python-dateutil-1.5:\n",
      "      Successfully uninstalled python-dateutil-1.5\n",
      "Successfully installed python-dateutil-2.6.1 s3fs-0.1.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import boto3\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import cStringIO\n",
    "#! sudo /mnt1/anaconda2/bin/pip install s3fs\n",
    "! sudo pip install s3fs\n",
    "import s3fs\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainSamples(viewers,videos,probab,viewerFeat,videoFeat,contxFeat):\n",
    "    trData = {} # trData = Dictionary with training data. This is histrory of viewer and video iteraction\n",
    "    X = {} # X  = Dictionary with viewer features as arrays\n",
    "    Y ={} # Y  = Dictionary with video features as arrays\n",
    "    for i in range(viewers):\n",
    "        X[i] = np.random.rand(viewerFeat)\n",
    "        a = 0 # timing of the video for a particular user,... \n",
    "                #to give the order in which the videos have been watched\n",
    "        for j in range(videos):\n",
    "            if int(np.random.binomial(1,probab ,1)[0]):\n",
    "                trData[(i,j,a)] = np.random.rand(contxFeat)\n",
    "                a+=1 # when a video is watched, we increase the value of a by 1 \n",
    "            if i==0:\n",
    "                Y[j] = np.random.rand(videoFeat)\n",
    "    return X,Y,trData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "viewers = 1000  #number of viewers\n",
    "videos =1000  #number of videos\n",
    "probab = 0.05  #probability of a viewer watching any one video\n",
    "viewerFeat = 310  #number of features describing a veiwer\n",
    "videoFeat = 300   #number of features describing a video\n",
    "contxFeat = 15 # number of contextual features\n",
    "# X  = Dictionary with viewer features as arrays\n",
    "# Y  = Dictionary with video features as arrays\n",
    "# trData = Dictionary with training data. This is histrory of viewer and video iteraction\n",
    "X ,Y ,trData = trainSamples(viewers ,videos ,probab ,viewerFeat ,videoFeat ,contxFeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_feat_inp = np.array([X[key] for key in sorted(X.keys())]) \n",
    "vid_feat_inp = np.array([Y[key] for key in sorted(Y.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_user = np.asarray(range(user_feat_inp.shape[0])).reshape(user_feat_inp.shape[0] ,1)\n",
    "key_vid = np.asarray(range(vid_feat_inp.shape[0])).reshape(vid_feat_inp.shape[0] ,1)\n",
    "user_feat_inp_w_key = np.concatenate((user_feat_inp ,key_user),axis = 1)\n",
    "vid_feat_inp_w_key = np.concatenate((vid_feat_inp ,key_vid),axis = 1)\n",
    "user_vid_time = trData.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bucket = 'c4-ati'\n",
    "dataLoc = 'ampapalika/rnn/dataHashed/'\n",
    "\n",
    "def dataLoad(bucket, dataLoc,valLoc):\n",
    "    '''\n",
    "    Function which returns list of all objects in s3 bucket path\n",
    "    '''\n",
    "    s3_client = boto3.client('s3')\n",
    "    path  = dataLoc+valLoc\n",
    "    response = s3_client.list_objects(Bucket = bucket,Prefix = path)\n",
    "    file_list = list()\n",
    "    for file in response['Contents']:\n",
    "        name = file['Key'].rsplit('/', 1)\n",
    "        file_list.append(name[1])\n",
    "        \n",
    "    fileKey= dataLoc+valLoc+'/'+file_list[1]\n",
    "    s3_client = boto3.client('s3')\n",
    "    obj = s3_client.get_object(Bucket='c4-ati', Key=fileKey)   \n",
    "    full_data = pd.DataFrame({0: []})#, 1: []\n",
    "    f = cStringIO.StringIO(obj['Body'].read())\n",
    "    \n",
    "    for df in pd.read_csv(f,header=None, chunksize=10000,dtype={0: int}):\n",
    "        full_data = pd.concat([full_data,df], ignore_index=True)\n",
    "    \n",
    "    return full_data\n",
    "\n",
    "\n",
    "def loadTrainDF(bucket, dataLoc,valLoc):\n",
    "   \n",
    "    full_data = dataLoad(bucket, dataLoc,valLoc)\n",
    "    full_data.rename(columns={1: 'movie_id',2: 'time_watch',3: 'rank',0: 'user_id'}, inplace=True)\n",
    "    del full_data['time_watch']\n",
    "    return full_data.astype('float32')\n",
    "\n",
    "def loadUserFeatDF(bucket, dataLoc, valLoc):\n",
    "    full_data = dataLoad(bucket, dataLoc,valLoc)\n",
    "    full_data.rename(columns={0: 'user_id'}, inplace=True)\n",
    "    return full_data.astype('float32')\n",
    "\n",
    "def loadVideoFeatDF(bucket, dataLoc, valLoc):\n",
    "    full_data = dataLoad(bucket, dataLoc,valLoc)\n",
    "    full_data.rename(columns={0: 'movie_id'}, inplace=True)\n",
    "    return full_data.astype('float32')\n",
    "\n",
    "user_feat_inp_w_key_df = loadUserFeatDF(bucket, dataLoc,'userFeatIndex2TGenre')\n",
    "vid_feat_inp_w_key_df = loadVideoFeatDF(bucket, dataLoc,'videoFeatGenre')\n",
    "user_vid_time_df = loadTrainDF(bucket,dataLoc, 'trainDataIndex2TU2TVGenre')\n",
    "\n",
    "user_vid_time_df_inp = user_vid_time_df\n",
    "for user in (np.array(user_vid_time_df_inp[\"user_id\"].unique())):\n",
    "    a = max(user_vid_time_df_inp.loc[user_vid_time_df_inp[\"user_id\"]== user, \"rank\"])\n",
    "    user_vid_time_df_inp = user_vid_time_df_inp[~((user_vid_time_df_inp[\"rank\"] == a) & (user_vid_time_df_inp[\"user_id\"] == user))]\n",
    "    \n",
    "num_user_feat, num_video_feat = user_feat_inp_w_key_df.shape[1] -1 , vid_feat_inp_w_key_df.shape[1] -1\n",
    "num_users, contex_feat = user_feat_inp_w_key_df.shape[0], len(user_vid_time_df.columns[3:])\n",
    "\n",
    "user_vid_time_df_sort=user_vid_time_df[[\"user_id\",'movie_id',\"rank\"]].sort_values([\"user_id\",\"rank\"])\n",
    "user_vid_time_df_sort = user_vid_time_df_sort.reset_index(drop=True)\n",
    "\n",
    "h=pd.get_dummies(user_vid_time_df_sort[\"movie_id\"],sparse=True,prefix='vid_')\n",
    "\n",
    "y_tr_p_w=pd.concat([user_vid_time_df_sort.reset_index(drop=True), h], axis=1)\n",
    "y_tr_p_w['desired'] = np.argmax(np.array(y_tr_p_w.iloc[:,3:]) ,1)\n",
    "y_tr_p_w = y_tr_p_w[y_tr_p_w[\"rank\"] > 3]\n",
    "\n",
    "\n",
    "max_watch = (y_tr_p_w.groupby('user_id',axis = 0).sum().iloc[:\n",
    "                                ,((list(np.where(y_tr_p_w.columns==\"rank\")[0])[0])):-1].sum(axis=1)).reset_index()\n",
    "\n",
    "user_w_vid_tim_and_feat = user_vid_time_df_inp.merge(user_feat_inp_w_key_df \n",
    "                                                 ,how = 'inner',on = 'user_id', sort = False)\n",
    "user_vid_time_vidfeat_usefit = user_w_vid_tim_and_feat.merge(vid_feat_inp_w_key_df \n",
    "                                                             ,how = 'inner' ,on='movie_id' ,sort = False)\n",
    "user_vid_time_vidfeat_usefit_sorted = user_vid_time_vidfeat_usefit.sort_values(['user_id', 'rank']) \n",
    "\n",
    "tr_users = num_users * 0.8\n",
    "tr_y = y_tr_p_w[y_tr_p_w['user_id'] < tr_users]\n",
    "te_y = y_tr_p_w[~y_tr_p_w['user_id'].isin(tr_y['user_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_feat_inp_w_key_df = pd.DataFrame(user_feat_inp_w_key) \n",
    "vid_feat_inp_w_key_df = pd.DataFrame(vid_feat_inp_w_key)\n",
    "user_vid_time_df = pd.DataFrame(user_vid_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rr = user_vid_time_df.sort_values([0 ,2]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = pd.get_dummies(rr[1],prefix = 'vid_')\n",
    "y_tr_p_w = pd.concat([rr.reset_index(drop = True), h], axis = 1)\n",
    "y_tr_p_w.rename(columns = {0: 'user'}, inplace = True)\n",
    "y_tr_p_w.rename(columns = {1: 'mov'}, inplace = True)\n",
    "y_tr_p_w.rename(columns = {2: 'rank'}, inplace = True)\n",
    "y_tr_p_w['desired'] = np.argmax(np.array(y_tr_p_w.iloc[:,4:]) ,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_watch = (y_tr_p_w.groupby('user',axis = 0).sum().iloc[:\n",
    "                                ,int(np.array(np.where(y_tr_p_w.columns=='rank'))):-1].sum(axis=1)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vid_feat_inp_w_key_df.rename(columns = {300: 'mov_id'}, inplace = True)\n",
    "user_feat_inp_w_key_df.rename(columns = {310: 'user_id'}, inplace = True)\n",
    "user_vid_time_df.rename(columns = {0: 'user_id'}, inplace = True)\n",
    "user_vid_time_df.rename(columns = {1: 'mov_id'}, inplace = True)\n",
    "user_w_vid_tim_and_feat = user_vid_time_df.merge(user_feat_inp_w_key_df \n",
    "                                                 ,how = 'inner',on = 'user_id', sort = False)\n",
    "user_vid_time_vidfeat_usefit = user_w_vid_tim_and_feat.merge(vid_feat_inp_w_key_df \n",
    "                                                             ,how = 'inner' ,on='mov_id' ,sort = False)\n",
    "user_vid_time_vidfeat_usefit.rename(columns={'2_x': 'time_watch'} ,inplace=True)\n",
    "user_vid_time_vidfeat_usefit_sorted = user_vid_time_vidfeat_usefit.sort_values(['user_id', 'time_watch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_users = viewers * 0.8\n",
    "tr_y = y_tr_p_w[y_tr_p_w['user'] < tr_users]\n",
    "te_y = y_tr_p_w[~y_tr_p_w['user'].isin(tr_y['user'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utility_matrix = y_tr_p_w.groupby('user').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utility_matrix = utility_matrix.reset_index().iloc[:,4:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_users = 1000\n",
    "num_video =1000\n",
    "top_k = 15\n",
    "top_r = 5\n",
    "top_l = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def minibatch_indices(X, minibatch_size):\n",
    "    minibatch_indices = np.arange(0, len(X), minibatch_size)\n",
    "    minibatch_indices = np.asarray(list(minibatch_indices) + [len(X)])\n",
    "    start_indices = minibatch_indices[:-1]\n",
    "    end_indices = minibatch_indices[1:]\n",
    "    return zip(start_indices, end_indices)\n",
    "\n",
    "\n",
    "def shuffle_in_unison(a, b):\n",
    "    \"\"\"\n",
    "    http://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "    \"\"\"\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "\n",
    "def PMF(X, rank=10, learning_rate=0.001, momentum=0.8,\n",
    "        regularization=0.25, minibatch_size=1000, max_epoch=1000,\n",
    "        nan_value=0, status_percentage=0.1, random_state=None):\n",
    "    \"\"\"\n",
    "    Python implementation of Probabilistic Matrix Factorization (PMF).\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: numpy array or scipy.sparse coo matrix, shape (n_users, n_items)\n",
    "        Input data. If a dense array is passed in, it will be converted to a\n",
    "        sparse matrix by looking for all `nan_value` numbers and treating them\n",
    "        as empty.\n",
    "    rank: int, optional (default=10)\n",
    "       Rank of the low-rank factor matrices. A higher rank should result in a\n",
    "       better approximation, at the cost of more memory and slower computataion.\n",
    "    learning_rate: float, optional (default=0.001)\n",
    "        Learning rate for minibatch gradient descent.\n",
    "    momentum: float, optional (default=0.8)\n",
    "        Momentum for minibatch gradient descent.\n",
    "    regularization: float, optional (default=0.25)\n",
    "        L2 regularization penalty for minibatch gradient descent.\n",
    "    minibatch_size: int, optional (default=1000)\n",
    "       The size of each minibatch. If this is larger than size of the dataset,\n",
    "       will default to running over the whole dataset.\n",
    "    max_epoch: int, optional (default=1000)\n",
    "        The maximum number of epochs.\n",
    "    nan_value: int, optional (default=0)\n",
    "        This value will be masked out of the input for calculations\n",
    "        Should match the value considered the \"not rated\" in the dataset X.\n",
    "    status_percentage: float in (0, 1), optional (default=0.1)\n",
    "        The relative percentage of `max_epochs` when status will be printed.\n",
    "        For example, 0.1 is every 10%, 0.01 is every 1%, and so on. For\n",
    "        the default values of max_epoch=1000, status_percentage=0.1 this\n",
    "        is equivalent to a status print every 100 epochs.\n",
    "    random_state: RandomState, int, or None, optional (default=None)\n",
    "        Random state to pass in. Can be an int, None, or np.random.RandomState\n",
    "        object.\n",
    "    Returns\n",
    "    -------\n",
    "    U: array-like, shape (X.shape[0], rank)\n",
    "        Row basis for reconstruction.\n",
    "        Usage:\n",
    "        reconstruction = np.dot(U, V.T) + X_mean\n",
    "    V: array-like, shape (X.shape[1], rank)\n",
    "        Column basis for reconstruction.\n",
    "        Usage:\n",
    "        reconstruction = np.dot(U, V.T) + X_mean\n",
    "    X_mean: float\n",
    "        Global mean prediction, needed for reconstruction\n",
    "        Usage\n",
    "        reconstruction = np.dot(U, V.T) + X_mean\n",
    "    Notes\n",
    "    -----\n",
    "    Based on code from Ruslan Salakhutdinov\n",
    "    http://www.cs.toronto.edu/~rsalakhu/code_BPMF/pmf.m\n",
    "    Probabilistic Matrix Factorization, R. Salakhutdinov and A. Mnih,\n",
    "    Advances in Neural Information Processing Systems 20, 2008\n",
    "    \"\"\"\n",
    "    if not sparse.isspmatrix_coo(X):\n",
    "        val_index = np.where(X != nan_value)\n",
    "        X = sparse.coo_matrix((X[val_index[0], val_index[1]],\n",
    "                               (val_index[0], val_index[1])))\n",
    "    # Simplest prediction is the global mean\n",
    "    X_mean = X.mean()\n",
    "    lr = learning_rate\n",
    "    reg = regularization\n",
    "    mom = momentum\n",
    "    if random_state is None or type(random_state) is int:\n",
    "        random_state = np.random.RandomState(random_state)\n",
    "    N, M = X.shape\n",
    "    U = 0.1 * random_state.randn(N, rank)\n",
    "    V = 0.1 * random_state.randn(M, rank)\n",
    "    U_inc = np.zeros_like(U)\n",
    "    V_inc = np.zeros_like(V)\n",
    "    dU = np.zeros_like(U)\n",
    "    dV = np.zeros_like(V)\n",
    "    epoch = 0\n",
    "    status_inc = int(np.ceil(max_epoch * status_percentage))\n",
    "    print(\"Printing updates every %i epochs\" % status_inc)\n",
    "    status_points = list(range(0, max_epoch, status_inc)) + [max_epoch - 1]\n",
    "    # Need this in order to index\n",
    "    X_s = X.tolil()\n",
    "    while epoch < max_epoch:\n",
    "        # Get indices for non-NaN values\n",
    "        r, c = X.nonzero()\n",
    "        mb_indices = minibatch_indices(zip(r, c), minibatch_size)\n",
    "        n_batches = len(mb_indices)\n",
    "        shuffle_in_unison(r, c)\n",
    "        mean_abs_err = 0.\n",
    "        for i, j in mb_indices:\n",
    "            # Reset derivative matrices each minibatch\n",
    "            dU[:, :] = 0.\n",
    "            dV[:, :] = 0.\n",
    "            # Slice out row and column indices\n",
    "            r_i = r[i:j]\n",
    "            c_i = c[i:j]\n",
    "            # Get data corresponding to the row and column indices\n",
    "            X_i = X_s[r_i, c_i].toarray().ravel() - X_mean\n",
    "            # Compute predictions\n",
    "            pred = np.sum(U[r_i] * V[c_i], axis=1)\n",
    "            # Compute how algorithm is doing\n",
    "            mean_abs_err += np.sum(np.abs(pred - X_i)) / (n_batches * (j - i))\n",
    "            # Loss has a tendency to be unstable, but is the \"right thing\"\n",
    "            # to monitor instead of sum_abs_err\n",
    "            # pred_loss = (pred - X_i) ** 2\n",
    "            # Compute gradients\n",
    "            grad_loss = 2 * (pred - X_i)\n",
    "            grad_U = grad_loss[:, None] * V[c_i] + reg * U[r_i]\n",
    "            grad_V = grad_loss[:, None] * U[r_i] + reg * V[c_i]\n",
    "            dU[r_i] = grad_U\n",
    "            dV[c_i] = grad_V\n",
    "            # Momentum storage\n",
    "            U_inc = mom * U_inc + lr * dU\n",
    "            V_inc = mom * V_inc + lr * dV\n",
    "            U -= U_inc\n",
    "            V -= V_inc\n",
    "        if epoch in status_points:\n",
    "            print(\"Epoch %i of %i\" % (epoch + 1, max_epoch))\n",
    "            print(\"Mean absolute error %f\" % (mean_abs_err))\n",
    "        epoch += 1\n",
    "    return U, V, X_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing updates every 100 epochs\n",
      "Epoch 1 of 1000\n",
      "Mean absolute error 0.947134\n",
      "Epoch 101 of 1000\n",
      "Mean absolute error 0.125000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-3c42cc227cec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     U, V, m = PMF(np.array(utility_matrix), learning_rate=0.001, momentum = 0.95,\n\u001b[0;32m----> 4\u001b[0;31m                   minibatch_size=50, rank = 100, max_epoch = 1000, random_state=1999)\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mR2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-fdb51b2d789d>\u001b[0m in \u001b[0;36mPMF\u001b[0;34m(X, rank, learning_rate, momentum, regularization, minibatch_size, max_epoch, nan_value, status_percentage, random_state)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mc_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;31m# Get data corresponding to the row and column indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mX_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mX_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0;31m# Compute predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/tensorflow/lib/python2.7/site-packages/scipy/sparse/lil.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_row_ranges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlil_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/tensorflow/lib/python2.7/site-packages/scipy/sparse/sputils.pyc\u001b[0m in \u001b[0;36m_index_to_arrays\u001b[0;34m(self, i, j)\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slicetoarange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/tensorflow/lib/python2.7/site-packages/numpy/core/shape_base.pyc\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/tensorflow/lib/python2.7/site-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import matplotlib.pyplot as plt\n",
    "    U, V, m = PMF(np.array(utility_matrix), learning_rate=0.001, momentum = 0.95,\n",
    "                  minibatch_size=50, rank = 100, max_epoch = 1000, random_state=1999)\n",
    "    R2 = np.dot(U, V.T) + m\n",
    "    plt.matshow(R * (R > 0))\n",
    "    plt.title(\"Ground truth ratings\")\n",
    "    plt.matshow(R2 * (R > 0))\n",
    "    plt.title(\"Predicted ratings\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
