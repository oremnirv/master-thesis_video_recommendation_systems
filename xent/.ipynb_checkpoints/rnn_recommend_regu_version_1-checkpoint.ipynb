{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><u>RNN thesis</u></h1>\n",
    "<h4>Omer Nivron</h4>\n",
    "15098443"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Package loading</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys as sys\n",
    "import random as rd\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting s3fs\n",
      "  Downloading s3fs-0.1.2.tar.gz\n",
      "Requirement already satisfied: boto3 in /usr/lib/python2.7/dist-packages (from s3fs)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/lib/python2.7/dist-packages (from boto3->s3fs)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/lib/python2.7/dist-packages (from boto3->s3fs)\n",
      "Requirement already satisfied: botocore<1.6.0,>=1.5.0 in /usr/lib/python2.7/dist-packages (from boto3->s3fs)\n",
      "Requirement already satisfied: futures<4.0.0,>=2.2.0; python_version == \"2.6\" or python_version == \"2.7\" in /usr/lib/python2.7/dist-packages (from s3transfer<0.2.0,>=0.1.10->boto3->s3fs)\n",
      "Requirement already satisfied: docutils>=0.10 in /usr/lib/python2.7/dist-packages (from botocore<1.6.0,>=1.5.0->boto3->s3fs)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<1.6.0,>=1.5.0->boto3->s3fs)\n",
      "  Downloading python_dateutil-2.6.1-py2.py3-none-any.whl (194kB)\n",
      "\u001b[K    100% |████████████████████████████████| 194kB 2.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python2.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.6.0,>=1.5.0->boto3->s3fs)\n",
      "Building wheels for collected packages: s3fs\n",
      "  Running setup.py bdist_wheel for s3fs ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ad/85/31/07e4a6c225e249b5e55116d9dcf94282c14e7f8fcc65969d22\n",
      "Successfully built s3fs\n",
      "Installing collected packages: s3fs, python-dateutil\n",
      "  Found existing installation: python-dateutil 1.5\n",
      "    Uninstalling python-dateutil-1.5:\n",
      "      Successfully uninstalled python-dateutil-1.5\n",
      "Successfully installed python-dateutil-2.6.1 s3fs-0.1.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import boto3\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import cStringIO\n",
    "#! sudo /mnt1/anaconda2/bin/pip install s3fs\n",
    "! sudo pip install s3fs\n",
    "import s3fs\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def range_bet_col_t_col_n_append(col_1 ,col_2):\n",
    "    app_ranges=[]\n",
    "    for i in range(col_1.shape[0]):\n",
    "        single_range = range((col_1[i]).astype(int), (col_2[i]).astype(int))\n",
    "        app_ranges = np.append(app_ranges, single_range)\n",
    "    return(app_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_pd_df_by_ext_vec(df, ext_sor_vec, cols):\n",
    "    df_s = df[((df[cols[0]]).astype(int)).isin(ext_sor_vec)] #\n",
    "    df_s['sort_cat'] = pd.Categorical(df_s[cols[0]],categories = ext_sor_vec,ordered = True)\n",
    "    if len(cols) > 1:\n",
    "        df_s.sort_values(['sort_cat',cols[1]] ,inplace = True)\n",
    "    \n",
    "    else:\n",
    "        df_s.sort_values(['sort_cat'],inplace = True) \n",
    "    \n",
    "    df_s.reset_index(inplace = True)\n",
    "    df_ = df_s.drop(['sort_cat','index'] ,axis = 1)\n",
    "    \n",
    "    return(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_model(model,n_hidden,layers):\n",
    "        # Define a lstm/GRU cell with tensorflow\n",
    "    if (layers==1):\n",
    "        if(model=='lstm'):\n",
    "            cell=tf.nn.rnn_cell.LSTMCell(n_hidden,state_is_tuple=True)\n",
    "        else:\n",
    "            cell=tf.nn.rnn_cell.GRUCell(n_hidden)   \n",
    "    else:\n",
    "        if(model=='lstm'):\n",
    "            lstm=tf.nn.rnn_cell.LSTMCell(n_hidden,state_is_tuple=True)\n",
    "            cell=tf.nn.rnn_cell.MultiRNNCell([lstm]*layers)\n",
    "\n",
    "        else:\n",
    "            gru=tf.nn.rnn_cell.GRUCell(n_hidden)\n",
    "            cell=tf.nn.rnn_cell.MultiRNNCell([gru]*layers)\n",
    "    return(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def algeb_geom_series(mode ,start ,jump ,length):\n",
    "    u = np.empty((length,))\n",
    "    u[0] = start\n",
    "    u[1:] = jump\n",
    "    if (mode == 0):\n",
    "        series=np.cumsum(u)\n",
    "    if (mode == 1):\n",
    "        series=np.cumprod(u)\n",
    "    return(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def np_pad_tr_x(x_tr, batch_size, str_idx, zero_array_x, length_vec):\n",
    "    start = 0\n",
    "    for i in range(batch_size):\n",
    "        if i > 0:\n",
    "            end = end + length_vec[i] \n",
    "        else:\n",
    "            end = length_vec[0]\n",
    "        zero_array_x[ str_idx[i] : (str_idx[i] + length_vec[i]) ] = x_tr[ start : end ] \n",
    "        start = end\n",
    "\n",
    "    return(zero_array_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainSamples(viewers,videos,probab,viewerFeat,videoFeat,contxFeat):\n",
    "    trData = {} # trData = Dictionary with training data. This is histrory of viewer and video iteraction\n",
    "    X = {} # X  = Dictionary with viewer features as arrays\n",
    "    Y ={} # Y  = Dictionary with video features as arrays\n",
    "    for i in range(viewers):\n",
    "        X[i] = np.random.rand(viewerFeat)\n",
    "        a = 0 # timing of the video for a particular user,... \n",
    "                #to give the order in which the videos have been watched\n",
    "        for j in range(videos):\n",
    "            if int(np.random.binomial(1,probab ,1)[0]):\n",
    "                trData[(i,j,a)] = np.random.rand(contxFeat)\n",
    "                a+=1 # when a video is watched, we increase the value of a by 1 \n",
    "            if i==0:\n",
    "                Y[j] = np.random.rand(videoFeat)\n",
    "    return X,Y,trData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "viewers = 1000  #number of viewers\n",
    "videos = 1000  #number of videos\n",
    "probab = 0.3  #probability of a viewer watching any one video\n",
    "viewerFeat = 310  #number of features describing a veiwer\n",
    "videoFeat = 300   #number of features describing a video\n",
    "contxFeat = 15 # number of contextual features\n",
    "# X  = Dictionary with viewer features as arrays\n",
    "# Y  = Dictionary with video features as arrays\n",
    "# trData = Dictionary with training data. This is histrory of viewer and video iteraction\n",
    "X,Y,trData = trainSamples(viewers,videos,probab,viewerFeat,videoFeat,contxFeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userFiltNum = 1000\n",
    "user_feat_inp_w_key_df = user_feat_inp_w_key_df2[user_feat_inp_w_key_df2['user_id'] < userFiltNum]\n",
    "user_vid_time_df  = user_vid_time_df2[user_vid_time_df2['user_id'] < userFiltNum]\n",
    "num_user_feat, num_video_feat = user_feat_inp_w_key_df.shape[1] -1 , vid_feat_inp_w_key_df.shape[1] -1\n",
    "num_users, contex_feat = user_feat_inp_w_key_df.shape[0], len(user_vid_time_df.columns[3:])\n",
    "\n",
    "user_vid_time_df_sort=user_vid_time_df[[\"user_id\",'movie_id',\"rank\"]].sort_values([\"user_id\",\"rank\"])\n",
    "user_vid_time_df_sort = user_vid_time_df_sort.reset_index(drop=True)\n",
    "\n",
    "h=pd.get_dummies(user_vid_time_df_sort[\"movie_id\"],sparse=True,prefix='vid_')\n",
    "\n",
    "y_tr_p_w=pd.concat([user_vid_time_df_sort.reset_index(drop=True), h], axis=1)\n",
    "y_tr_p_w['desired'] = np.argmax(np.array(y_tr_p_w.iloc[:,3:]) ,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform a dict to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_feat_inp=np.array([X[key] for key in sorted(X.keys())]) # dictionary to numpy array\n",
    "\n",
    "vid_feat_inp=np.array([Y[key] for key in sorted(Y.keys())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a column key specifying user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_user=np.asarray(range(user_feat_inp.shape[0])).reshape(user_feat_inp.shape[0],1)\n",
    "key_vid=np.asarray(range(vid_feat_inp.shape[0])).reshape(vid_feat_inp.shape[0],1)\n",
    "user_feat_inp_w_key=np.concatenate((user_feat_inp,key_user),axis=1)\n",
    "vid_feat_inp_w_key=np.concatenate((vid_feat_inp,key_vid),axis=1)\n",
    "user_vid_time=trData.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform numpy to pandas df in order to use easy merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_feat_inp_w_key_df=pd.DataFrame(user_feat_inp_w_key) # numpy array to Data frame\n",
    "vid_feat_inp_w_key_df=pd.DataFrame(vid_feat_inp_w_key)#d\n",
    "user_vid_time_df=pd.DataFrame(user_vid_time)# b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct true_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rr = user_vid_time_df.sort_values([0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = pd.get_dummies(rr[1],prefix='vid_')\n",
    "y_tr = pd.concat([rr.reset_index(drop = True), h], axis=1)\n",
    "y_tr.rename(columns = {0: 'user'}, inplace = True)\n",
    "y_tr.rename(columns = {1: 'mov'}, inplace = True)\n",
    "y_tr.rename(columns = {2: 'rank'}, inplace = True)\n",
    "y_tr['desired'] = np.argmax(np.array(y_tr.iloc[:,4:]) ,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge two pandas df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_watch = (y_tr.groupby('user',axis = 0).sum().iloc[:\n",
    "                                ,int(np.array(np.where(y_tr.columns=='rank'))):-1].sum(axis=1)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vid_feat_inp_w_key_df.rename(columns={300: 'mov_id'}, inplace=True)\n",
    "user_feat_inp_w_key_df.rename(columns={310: 'user_id'}, inplace=True)\n",
    "user_vid_time_df.rename(columns={0: 'user_id'}, inplace=True)\n",
    "user_vid_time_df.rename(columns={1: 'mov_id'}, inplace=True)\n",
    "user_w_vid_tim_and_feat=user_vid_time_df.merge(user_feat_inp_w_key_df, how='inner',on='user_id', sort=False)\n",
    "user_vid_time_vidfeat_usefit=user_w_vid_tim_and_feat.merge(vid_feat_inp_w_key_df,how='inner'\n",
    "                                                           ,on='mov_id',sort=False)\n",
    "user_vid_time_vidfeat_usefit.rename(columns={'2_x': 'rank'}, inplace=True)\n",
    "user_vid_time_vidfeat_usefit_sorted=user_vid_time_vidfeat_usefit.sort_values(['user_id', 'rank']) \n",
    "cols_to_del=[user_vid_time_vidfeat_usefit_sorted.columns.get_loc(\"user_id\")\n",
    "             ,user_vid_time_vidfeat_usefit_sorted.columns.get_loc(\"mov_id\")\n",
    "             ,user_vid_time_vidfeat_usefit_sorted.columns.get_loc(\"rank\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_users=viewers*0.8\n",
    "tr_y=y_tr[y_tr['user']<tr_users]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bucket = 'c4-ati'\n",
    "dataLoc = 'ampapalika/rnn/dataHashed/'\n",
    "\n",
    "def dataLoad(bucket, dataLoc,valLoc):\n",
    "    '''\n",
    "    Function which returns list of all objects in s3 bucket path\n",
    "    '''\n",
    "    s3_client = boto3.client('s3')\n",
    "    path  = dataLoc+valLoc\n",
    "    response = s3_client.list_objects(Bucket = bucket,Prefix = path)\n",
    "    file_list = list()\n",
    "    for file in response['Contents']:\n",
    "        name = file['Key'].rsplit('/', 1)\n",
    "        file_list.append(name[1])\n",
    "        \n",
    "    fileKey= dataLoc+valLoc+'/'+file_list[1]\n",
    "    s3_client = boto3.client('s3')\n",
    "    obj = s3_client.get_object(Bucket='c4-ati', Key=fileKey)   \n",
    "    full_data = pd.DataFrame({0: []})#, 1: []\n",
    "    f = cStringIO.StringIO(obj['Body'].read())\n",
    "    \n",
    "    for df in pd.read_csv(f,header=None, chunksize=10000,dtype={0: int}):\n",
    "        full_data = pd.concat([full_data,df], ignore_index=True)\n",
    "    \n",
    "    return full_data\n",
    "\n",
    "\n",
    "def loadTrainDF(bucket, dataLoc,valLoc):\n",
    "   \n",
    "    full_data = dataLoad(bucket, dataLoc,valLoc)\n",
    "    full_data.rename(columns={1: 'movie_id',2: 'time_watch',3: 'rank',0: 'user_id'}, inplace=True)\n",
    "    del full_data['time_watch']\n",
    "    return full_data.astype('float32')\n",
    "\n",
    "def loadUserFeatDF(bucket, dataLoc, valLoc):\n",
    "    full_data = dataLoad(bucket, dataLoc,valLoc)\n",
    "    full_data.rename(columns={0: 'user_id'}, inplace=True)\n",
    "    return full_data.astype('float32')\n",
    "\n",
    "def loadVideoFeatDF(bucket, dataLoc, valLoc):\n",
    "    full_data = dataLoad(bucket, dataLoc,valLoc)\n",
    "    full_data.rename(columns={0: 'movie_id'}, inplace=True)\n",
    "    return full_data.astype('float32')\n",
    "\n",
    "\n",
    "user_feat_inp_w_key_df = loadUserFeatDF(bucket, dataLoc,'userFeatIndex2TGenre')\n",
    "vid_feat_inp_w_key_df = loadVideoFeatDF(bucket, dataLoc,'videoFeatGenre')\n",
    "user_vid_time_df = loadTrainDF(bucket,dataLoc, 'trainDataIndex2TU2TVGenre')\n",
    "user_vid_time_df2NS = loadTrainDF(bucket,dataLoc, 'trainDataIndexTU2TVNSGenre')\n",
    "\n",
    "user_vid_time_df_inp = user_vid_time_df\n",
    "for user in (np.array(user_vid_time_df_inp[\"user_id\"].unique())):\n",
    "    a = max(user_vid_time_df_inp.loc[user_vid_time_df_inp[\"user_id\"]== user, \"rank\"])\n",
    "    user_vid_time_df_inp = user_vid_time_df_inp[~((user_vid_time_df_inp[\"rank\"] == a) & (user_vid_time_df_inp[\"user_id\"] == user))]\n",
    "\n",
    "num_user_feat, num_video_feat = user_feat_inp_w_key_df.shape[1] -1 , vid_feat_inp_w_key_df.shape[1] -1\n",
    "num_users, contex_feat = user_feat_inp_w_key_df.shape[0], len(user_vid_time_df.columns[3:])\n",
    "\n",
    "user_vid_time_df_sort=user_vid_time_df[[\"user_id\",'movie_id',\"rank\"]].sort_values([\"user_id\",\"rank\"])\n",
    "user_vid_time_df_sort = user_vid_time_df_sort.reset_index(drop=True)\n",
    "\n",
    "h=pd.get_dummies(user_vid_time_df_sort[\"movie_id\"],sparse=True,prefix='vid_')\n",
    "\n",
    "y_tr_p_w=pd.concat([user_vid_time_df_sort.reset_index(drop=True), h], axis=1)\n",
    "y_tr_p_w['desired'] = np.argmax(np.array(y_tr_p_w.iloc[:,3:]) ,1)\n",
    "y_tr_p_w = y_tr_p_w[y_tr_p_w[\"rank\"] > 3]\n",
    "\n",
    "max_watch = (y_tr_p_w.groupby('user_id',axis = 0).sum().iloc[:\n",
    "                                ,((list(np.where(y_tr_p_w.columns==\"rank\")[0])[0])):-1].sum(axis=1)).reset_index()\n",
    "\n",
    "user_w_vid_tim_and_feat = user_vid_time_df_inp.merge(user_feat_inp_w_key_df \n",
    "                                                 ,how = 'inner',on = 'user_id', sort = False)\n",
    "user_vid_time_vidfeat_usefit = user_w_vid_tim_and_feat.merge(vid_feat_inp_w_key_df \n",
    "                                                             ,how = 'inner' ,on='movie_id' ,sort = False)\n",
    "user_vid_time_vidfeat_usefit_sorted = user_vid_time_vidfeat_usefit.sort_values(['user_id', 'rank']) \n",
    "\n",
    "user_vid_time_df2NS_10 = user_vid_time_df2NS[user_vid_time_df2NS.user_id.isin(user_vid_time_df2NS[user_vid_time_df2NS['rank']==10]['user_id'])]\n",
    "user_vid_time_df2NS_top10 =user_vid_time_df2NS_10[user_vid_time_df2NS_10['rank']<=10]\n",
    "user_vid_time_df2NS_top10 = user_vid_time_df2NS_top10[['user_id','movie_id','rank']]\n",
    "\n",
    "_user_vid_time_df2NS_top10 = user_vid_time_df2NS_top10.merge(vid_feat_inp_w_key_df \n",
    "                                                             ,how = 'inner' ,on='movie_id' ,sort = False)\n",
    "\n",
    "_user_vid_time_df2NS_top10 = _user_vid_time_df2NS_top10.sort_values([\"user_id\", \"rank\"])\n",
    "\n",
    "train_dislike = _user_vid_time_df2NS_top10[:10000]\n",
    "\n",
    "test_dislike = _user_vid_time_df2NS_top10[10000:]\n",
    "\n",
    "user_tr_choice = train_dislike[\"user_id\"].unique()\n",
    "user_te_choice = test_dislike[\"user_id\"].unique()\n",
    "\n",
    "tr_y = y_tr_p_w[y_tr_p_w['user_id'].isin(user_tr_choice)]\n",
    "te_y = y_tr_p_w[y_tr_p_w['user_id'].isin(user_te_choice)]\n",
    "\n",
    "tr_users = len(user_tr_choice)\n",
    "te_users = len(te_y[\"user_id\"].unique())\n",
    "\n",
    "tr_likes = user_vid_time_vidfeat_usefit_sorted[user_vid_time_vidfeat_usefit_sorted[\"user_id\"]\n",
    "                                            .isin(user_tr_choice)]\n",
    "\n",
    "te_likes = user_vid_time_vidfeat_usefit_sorted[user_vid_time_vidfeat_usefit_sorted[\"user_id\"]\n",
    "                                            .isin(user_te_choice)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Configurations</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_opt_epoch=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LSTM with X units</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_opt_epoch = 1000\n",
    "model = 'lstm'\n",
    "layers = 1\n",
    "n_samples = 2000\n",
    "top_k = 15\n",
    "top_l = 10\n",
    "top_r = 5\n",
    "\n",
    "n_users = num_users\n",
    "te_users = int(np.ceil(num_users*0.2))\n",
    "n_feature = num_user_feat + num_video_feat + contex_feat #623\n",
    "lr_rat = 0.001\n",
    "num_video = h.shape[1]\n",
    "beta = 0.01\n",
    "\n",
    "if tr_users > 500:\n",
    "    batch_size = 500\n",
    "else:\n",
    "    batch_size  = tr_users\n",
    "        \n",
    "n_hidden = 64 # hidden layer num of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard saving paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! mkdir -p ./reco_rnn/tensor_plot/xent_genre/\n",
    "#! mkdir -p ./reco_rnn/xent_cost_genre/\n",
    "#logs_path = \"/Users/onivron/Desktop/reco_rnn/tensor_plot/p_w/\"\n",
    "logs_path = \"./reco_rnn/tensor_plot/xent_genre/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Var creation</h3>\n",
    "We build data placeholders & variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('input'):\n",
    "    dynam_input=tf.placeholder(\"float32\", [batch_size, None, n_feature], name = 'dynam_input') \n",
    "    y_true = tf.placeholder(\"int32\", [None, 1], name = 'Input_y')\n",
    "    tr_rw = tf.placeholder(\"int32\" ,[None, 1] ,name = '_rw')\n",
    "    max_batch_length = tf.placeholder(\"float32\", [batch_size], name = 'max_leng')\n",
    "\n",
    "with tf.name_scope(\"weights\"):\n",
    "    W = {'out_2':tf.Variable(tf.random_normal([n_hidden, num_video]), name = 'w_2')}  \n",
    "with tf.name_scope(\"biases\"):\n",
    "    b = {'out_2':tf.Variable(tf.random_normal([num_video]), name='b_2')} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(dynam_input, max_batch_length, W, b, model):\n",
    "    lstm_cell = rnn_model(model, n_hidden, layers)\n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cell,inputs = dynam_input\n",
    "                                     ,dtype = tf.float32, sequence_length = max_batch_length)\n",
    "    out_shaped = tf.reshape(outputs,[-1, n_hidden]) \n",
    "    lay_2 = tf.matmul(out_shaped, W['out_2']) + b['out_2'] \n",
    "    lay_3 = tf.sigmoid(lay_2)\n",
    "    \n",
    "    return (lay_2, lay_3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define loss, accuracy & optimizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred, sig_y_pred = RNN(dynam_input, max_batch_length, W, b, model)\n",
    "\n",
    "relevant_y_pred = tf.reshape(tf.gather(params = tf.reshape(y_pred\n",
    "                                        ,[-1 ,num_video]), indices = tr_rw), [-1, num_video])\n",
    "\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = relevant_y_pred, \n",
    "                            labels = tf.reshape(y_true, [-1])))   \n",
    "    regularizer = tf.nn.l2_loss(tf.abs(W['out_2']))\n",
    "    xent_regu = tf.reduce_mean(xent + beta * regularizer)\n",
    "\n",
    "top_vals, top_indice = tf.nn.top_k(relevant_y_pred, n_samples)\n",
    "    \n",
    "with tf.name_scope('Accuracy_top_r'):\n",
    "    top_15_indx_r = tf.slice(top_indice,[0,0],[-1, top_r])\n",
    "    to_bool_r = tf.reduce_sum(tf.cast(tf.equal(y_true, top_15_indx_r), tf.float32), 1)\n",
    "    accuracy_r = tf.reduce_mean(to_bool_r)   \n",
    "    \n",
    "with tf.name_scope('Accuracy_top_l'):\n",
    "    top_15_indx_l = tf.slice(top_indice,[0,0],[-1, top_l])\n",
    "    to_bool_l = tf.reduce_sum(tf.cast(tf.equal(y_true, top_15_indx_l), tf.float32), 1)\n",
    "    accuracy_l = tf.reduce_mean(to_bool_l)\n",
    "    \n",
    "with tf.name_scope('Accuracy_top_k'):\n",
    "    top_15_indx_k = tf.slice(top_indice,[0,0],[-1,top_k])\n",
    "    to_bool_k = tf.reduce_sum(tf.cast(tf.equal(y_true, top_15_indx_k), tf.float32), 1)\n",
    "    accuracy_k = tf.reduce_mean(to_bool_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('train'):    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = lr_rat).minimize(xent_regu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tensorboard set-up</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.summary.scalar(\"cost\", xent_regu)\n",
    "tf.summary.scalar(\"Accuracy_top_r\", accuracy_r)\n",
    "tf.summary.scalar(\"Accuracy_top_k\", accuracy_k)\n",
    "tf.summary.scalar(\"Accuracy_top_l\", accuracy_l)\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "writer_opt = tf.summary.FileWriter(logs_path,graph = tf.get_default_graph())\n",
    "saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training - optimization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session() \n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./reco_rnn/xent_cost_genre/accu\n"
     ]
    }
   ],
   "source": [
    "save_MDir = './reco_rnn/xent_cost_genre/'\n",
    "save_model = os.path.join(save_MDir,'accu')\n",
    "sess=tf.Session() \n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver.restore(sess = sess, save_path= save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/lib/python2.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/usr/lib/python2.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/lib/python2.7/dist-packages/ipykernel_launcher.py:23: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 0 loss: 23.5061 accuracy 15: 0.183435\n",
      "epoch number: 0 accuracy 10: 0.160824\n",
      "epoch number: 0 accuracy 5: 0.127457\n",
      "epoch number: 0 loss: 23.4387 accuracy 15: 0.193562\n",
      "epoch number: 0 accuracy 10: 0.169406\n",
      "epoch number: 0 accuracy 5: 0.133023\n",
      "epoch number: 0 loss: 23.418 accuracy 15: 0.190195\n",
      "epoch number: 0 accuracy 10: 0.167249\n",
      "epoch number: 0 accuracy 5: 0.131238\n",
      "epoch number: 10 loss: 22.6188 accuracy 15: 0.19357\n",
      "epoch number: 10 accuracy 10: 0.170197\n",
      "epoch number: 10 accuracy 5: 0.134096\n",
      "epoch number: 10 loss: 22.6015 accuracy 15: 0.184069\n",
      "epoch number: 10 accuracy 10: 0.160945\n",
      "epoch number: 10 accuracy 5: 0.125268\n",
      "epoch number: 10 loss: 22.5545 accuracy 15: 0.188419\n",
      "epoch number: 10 accuracy 10: 0.165009\n",
      "epoch number: 10 accuracy 5: 0.129578\n",
      "epoch number: 20 loss: 21.786 accuracy 15: 0.185784\n",
      "epoch number: 20 accuracy 10: 0.162492\n",
      "epoch number: 20 accuracy 5: 0.127777\n",
      "epoch number: 20 loss: 21.795 accuracy 15: 0.180376\n",
      "epoch number: 20 accuracy 10: 0.158\n",
      "epoch number: 20 accuracy 5: 0.123752\n",
      "epoch number: 20 loss: 21.7707 accuracy 15: 0.182402\n",
      "epoch number: 20 accuracy 10: 0.159799\n",
      "epoch number: 20 accuracy 5: 0.124506\n",
      "epoch number: 150 loss: 14.4335 accuracy 15: 0.171415\n",
      "epoch number: 150 accuracy 10: 0.148653\n",
      "epoch number: 150 accuracy 5: 0.113009\n",
      "epoch number: 150 loss: 14.4162 accuracy 15: 0.169134\n",
      "epoch number: 150 accuracy 10: 0.146741\n",
      "epoch number: 150 accuracy 5: 0.111832\n",
      "epoch number: 150 loss: 14.373 accuracy 15: 0.171105\n",
      "epoch number: 150 accuracy 10: 0.148497\n",
      "epoch number: 150 accuracy 5: 0.113987\n",
      "epoch number: 160 loss: 14.0239 accuracy 15: 0.17529\n",
      "epoch number: 160 accuracy 10: 0.151732\n",
      "epoch number: 160 accuracy 5: 0.115657\n",
      "epoch number: 160 loss: 14.0105 accuracy 15: 0.174442\n",
      "epoch number: 160 accuracy 10: 0.150887\n",
      "epoch number: 160 accuracy 5: 0.114362\n",
      "epoch number: 160 loss: 13.9925 accuracy 15: 0.179249\n",
      "epoch number: 160 accuracy 10: 0.155779\n",
      "epoch number: 160 accuracy 5: 0.119045\n",
      "epoch number: 170 loss: 13.6556 accuracy 15: 0.182526\n",
      "epoch number: 170 accuracy 10: 0.158343\n",
      "epoch number: 170 accuracy 5: 0.120729\n",
      "epoch number: 170 loss: 13.6644 accuracy 15: 0.172083\n",
      "epoch number: 170 accuracy 10: 0.148815\n",
      "epoch number: 170 accuracy 5: 0.113085\n",
      "epoch number: 170 loss: 13.6931 accuracy 15: 0.162702\n",
      "epoch number: 170 accuracy 10: 0.140892\n",
      "epoch number: 170 accuracy 5: 0.107903\n",
      "epoch number: 180 loss: 13.3173 accuracy 15: 0.180366\n",
      "epoch number: 180 accuracy 10: 0.156826\n",
      "epoch number: 180 accuracy 5: 0.119153\n",
      "epoch number: 180 loss: 13.3438 accuracy 15: 0.172952\n",
      "epoch number: 180 accuracy 10: 0.150133\n",
      "epoch number: 180 accuracy 5: 0.113404\n",
      "epoch number: 180 loss: 13.3249 accuracy 15: 0.170702\n",
      "epoch number: 180 accuracy 10: 0.148464\n",
      "epoch number: 180 accuracy 5: 0.112757\n",
      "epoch number: 190 loss: 13.0477 accuracy 15: 0.169404\n",
      "epoch number: 190 accuracy 10: 0.14764\n",
      "epoch number: 190 accuracy 5: 0.112282\n",
      "epoch number: 190 loss: 13.0236 accuracy 15: 0.174235\n",
      "epoch number: 190 accuracy 10: 0.151728\n",
      "epoch number: 190 accuracy 5: 0.114928\n",
      "epoch number: 190 loss: 13.0121 accuracy 15: 0.174168\n",
      "epoch number: 190 accuracy 10: 0.151618\n",
      "epoch number: 190 accuracy 5: 0.114547\n",
      "epoch number: 200 loss: 12.7208 accuracy 15: 0.179142\n",
      "epoch number: 200 accuracy 10: 0.156203\n",
      "epoch number: 200 accuracy 5: 0.118089\n",
      "epoch number: 200 loss: 12.7309 accuracy 15: 0.172979\n",
      "epoch number: 200 accuracy 10: 0.150083\n",
      "epoch number: 200 accuracy 5: 0.113198\n",
      "epoch number: 200 loss: 12.7151 accuracy 15: 0.170353\n",
      "epoch number: 200 accuracy 10: 0.1484\n",
      "epoch number: 200 accuracy 5: 0.112794\n",
      "epoch number: 210 loss: 12.461 accuracy 15: 0.173161\n",
      "epoch number: 210 accuracy 10: 0.150326\n",
      "epoch number: 210 accuracy 5: 0.113785\n",
      "epoch number: 210 loss: 12.421 accuracy 15: 0.176794\n",
      "epoch number: 210 accuracy 10: 0.153411\n",
      "epoch number: 210 accuracy 5: 0.115669\n",
      "epoch number: 210 loss: 12.4228 accuracy 15: 0.178644\n",
      "epoch number: 210 accuracy 10: 0.155574\n",
      "epoch number: 210 accuracy 5: 0.11694\n",
      "epoch number: 220 loss: 12.183 accuracy 15: 0.176207\n",
      "epoch number: 220 accuracy 10: 0.153052\n",
      "epoch number: 220 accuracy 5: 0.114779\n",
      "epoch number: 220 loss: 12.1726 accuracy 15: 0.174624\n",
      "epoch number: 220 accuracy 10: 0.151811\n",
      "epoch number: 220 accuracy 5: 0.113471\n",
      "epoch number: 220 loss: 12.1792 accuracy 15: 0.171319\n",
      "epoch number: 220 accuracy 10: 0.150059\n",
      "epoch number: 220 accuracy 5: 0.112207\n"
     ]
    }
   ],
   "source": [
    "with open('xent_genre_accuracy.csv', 'wb') as csvfile:\n",
    "    wr = csv.writer(csvfile, delimiter='\\t', lineterminator='\\n')\n",
    "    for epoch in range(int(n_opt_epoch)):\n",
    "        if(epoch != 0):\n",
    "            writer_opt.add_summary(summary,epoch)\n",
    "        for batch_n in range(int(n_users/batch_size)):   \n",
    "            rw_to_chose = np.random.choice((tr_likes[\"user_id\"].unique()), batch_size,replace = False) \n",
    "            x_tr = sort_pd_df_by_ext_vec(tr_likes, rw_to_chose, cols = ['user_id', 'rank'])\n",
    "            y_batch = sort_pd_df_by_ext_vec(tr_y, rw_to_chose,cols = ['user_id', 'rank'])\n",
    "            length_vec = sort_pd_df_by_ext_vec(df = max_watch, ext_sor_vec = rw_to_chose, cols = ['user_id'])      \n",
    "            length_max = max(length_vec.iloc[:, 1])\n",
    "            trial_size = (int(batch_size * length_max), n_feature)\n",
    "            str_idx = algeb_geom_series(0 ,start = 0 ,jump = length_max ,length = batch_size)\n",
    "            end_idx = np.append(length_vec.iloc[0, 1], length_vec.iloc[1:, 1:] + str_idx[1:].reshape(batch_size-1, -1))\n",
    "            app_range = range_bet_col_t_col_n_append(str_idx, end_idx)\n",
    "            zero_array_x = np.zeros(trial_size)\n",
    "            _x_tr = np_pad_tr_x(x_tr.iloc[:,3:], batch_size, str_idx.astype(int),\n",
    "                                            zero_array_x, length_vec.iloc[:,1].astype(int))      \n",
    "            acc_r, acc_l, acc_k, xent, _, summary = sess.run([accuracy_r, accuracy_l, accuracy_k\n",
    "                                                              , xent_regu, optimizer, summary_op]\n",
    "                                         ,feed_dict = {dynam_input: _x_tr.reshape(batch_size, -1, n_feature)\n",
    "                                         ,tr_rw: app_range.reshape(-1, 1)\n",
    "                                         ,y_true: y_batch['desired'].reshape(-1, 1)\n",
    "                                         ,max_batch_length: length_vec.iloc[:, 1]\n",
    "                                         })\n",
    "            wr.writerow([acc_r])\n",
    "            wr.writerow([acc_l])\n",
    "            wr.writerow([acc_k])\n",
    "            if (epoch%10 == 0):\n",
    "                print('epoch number:', epoch, 'loss:', xent, 'accuracy 15:', acc_k)\n",
    "                print('epoch number:', epoch, 'accuracy 10:', acc_l)\n",
    "                print('epoch number:', epoch, 'accuracy 5:', acc_r)\n",
    "\n",
    "        folder='./reco_rnn/xent_cost_genre/'\n",
    "        save_path=saver.save(sess,folder+'accu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "6/7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>End session</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
