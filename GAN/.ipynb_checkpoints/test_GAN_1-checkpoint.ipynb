{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys as sys\n",
    "import random as rd\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_opt_epoch = 2\n",
    "model = 'lstm'\n",
    "layers = 1\n",
    "n_samples = 10\n",
    "n_users = 1000\n",
    "te_users = int(n_users * 0.2)\n",
    "n_feature = 300\n",
    "n_user_feature = 310\n",
    "lr_rat = 0.001\n",
    "num_video = 1000\n",
    "beta = 0.01\n",
    "batch_size = 128\n",
    "n_hidden = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_tri_mat(length, batch_size):\n",
    "    upper_triangular_ones = np.float32(np.triu(np.ones((length, length))))\n",
    "    repeated_tri = np.float32(np.kron(np.eye(batch_size), upper_triangular_ones))\n",
    "    return repeated_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negtv_sample(to_sample, pop, batch_size, length_max, num_to_sample):\n",
    "    num_rw_neg_mtrx = length_max * batch_size\n",
    "    s_neg = (num_rw_neg_mtrx, num_to_sample)\n",
    "    neg_sample = np.zeros(s_neg)\n",
    "    start = 0\n",
    "    for i in range(batch_size):\n",
    "        not_watch = (np.array(to_sample.iloc[i,1])).reshape(-1)\n",
    "        pop_not_watch_norm = pop[not_watch]/sum(pop[not_watch])\n",
    "        if (i > 0):\n",
    "            end = length_max + end\n",
    "        else:\n",
    "            end = length_max\n",
    "        neg_sample[start:end] = np.random.choice(not_watch ,(length_max ,num_to_sample)\n",
    "                                               ,p = pop_not_watch_norm ,replace = False)\n",
    "        start = np.copy(end)\n",
    "    # get for each row the column to pick      \n",
    "    rep_seq = np.repeat(range(neg_sample.shape[0]) ,num_to_sample).reshape(-1,1) \n",
    "    idx_p_w = np.concatenate((rep_seq ,neg_sample.reshape(-1,1)) ,axis = 1)\n",
    "    \n",
    "    return(neg_sample ,idx_p_w ,rep_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def np_pad_tr_x(x_tr, batch_size, str_idx, zero_array_x, length_vec):\n",
    "    start = 0\n",
    "    for i in range(batch_size):\n",
    "        if i > 0:\n",
    "            end = end + length_vec[i] \n",
    "        else:\n",
    "            end = length_vec[0]\n",
    "        zero_array_x[ str_idx[i] : (str_idx[i] + length_vec[i]) ] = x_tr[ start : end ] \n",
    "        start = end\n",
    "    return(zero_array_x)\n",
    "\n",
    "\n",
    "def np_pad_tr_x_n_tr_y(x_tr, y_tr, batch_size, str_idx, zero_array_x, zero_array_y, length_vec):\n",
    "    start = 0\n",
    "    for i in range(batch_size):\n",
    "        if i > 0:\n",
    "            end = end + length_vec[i] \n",
    "        else:\n",
    "            end = length_vec[0]\n",
    "        zero_array_x[ str_idx[i] : (str_idx[i] + length_vec[i]) ] = x_tr[ start : end ] \n",
    "        zero_array_y[ str_idx[i] : (str_idx[i] + length_vec[i]) ] = y_tr.iloc[ start : end, (y_tr.shape[1] - 1) ] \n",
    "        start = end\n",
    "\n",
    "    return(zero_array_x, zero_array_y)\n",
    "\n",
    "\n",
    "def range_bet_col_t_col_n_append(col_1 ,col_2):\n",
    "    app_ranges=[]\n",
    "    for i in range(col_1.shape[0]):\n",
    "        single_range = range((col_1[i]).astype(int) ,(col_2[i]).astype(int))\n",
    "        app_ranges = np.append(app_ranges,single_range)\n",
    "    return(app_ranges)\n",
    "\n",
    "\n",
    "def algeb_geom_series(mode ,start ,jump ,length):\n",
    "    u = np.empty((length,))\n",
    "    u[0] = start\n",
    "    u[1:] = jump\n",
    "    if (mode == 0):\n",
    "        series=np.cumsum(u)\n",
    "    if (mode == 1):\n",
    "        series=np.cumprod(u)\n",
    "    return(series)\n",
    "\n",
    "def sort_pd_df_by_ext_vec(df,ext_sor_vec,cols):\n",
    "    df_s = df[((df[cols[0]]).astype(int)).isin(ext_sor_vec)] #\n",
    "    df_s['sort_cat'] = pd.Categorical(df_s[cols[0]],categories = ext_sor_vec,ordered = True)\n",
    "    if len(cols) > 1:\n",
    "        df_s.sort_values(['sort_cat',cols[1]] ,inplace = True)\n",
    "    \n",
    "    else:\n",
    "        df_s.sort_values(['sort_cat'],inplace = True) \n",
    "    \n",
    "    df_s.reset_index(inplace = True)\n",
    "    df_ = df_s.drop(['sort_cat','index'] ,axis = 1)\n",
    "    \n",
    "    return(df_)\n",
    "\n",
    "def trainSamples(viewers,videos,probab,viewerFeat,videoFeat,contxFeat):\n",
    "    trData = {} # trData = Dictionary with training data. This is histrory of viewer and video iteraction\n",
    "    X = {} # X  = Dictionary with viewer features as arrays\n",
    "    Y ={} # Y  = Dictionary with video features as arrays\n",
    "    for i in range(viewers):\n",
    "        X[i] = np.random.rand(viewerFeat)\n",
    "        a = 0 # timing of the video for a particular user,... \n",
    "                #to give the order in which the videos have been watched\n",
    "        for j in range(videos):\n",
    "            if int(np.random.binomial(1,probab ,1)[0]):\n",
    "                trData[(i,j,a)] = np.random.rand(contxFeat)\n",
    "                a+=1 # when a video is watched, we increase the value of a by 1 \n",
    "            if i==0:\n",
    "                Y[j] = np.random.rand(videoFeat)\n",
    "    return X,Y,trData\n",
    "\n",
    "\n",
    "def attach_zeros_to_np_arr(array ,size_to_att ,axis):\n",
    "    size = size_to_att\n",
    "    if(axis == 0):\n",
    "        new_arr = np.append(array ,np.zeros(size) ,axis = 0)\n",
    "    if(axis == 1):\n",
    "        new_arr = np.concatenate((array ,np.zeros(size)) ,axis = 1)\n",
    "    return(new_arr)\n",
    "\n",
    "\n",
    "def rnn_model(model ,n_hidden ,layers):\n",
    "    if (layers == 1):\n",
    "        if(model == 'lstm'):\n",
    "            try:\n",
    "                cell = tf.nn.rnn_cell.LSTMCell(n_hidden ,state_is_tuple=True,reuse=tf.get_variable_scope().reuse)\n",
    "            except ValueError:\n",
    "                print('yo')\n",
    "                cell = tf.nn.rnn_cell.LSTMCell(n_hidden ,state_is_tuple=True, reuse=True)\n",
    "        else:\n",
    "            cell = tf.nn.rnn_cell.GRUCell(n_hidden)   \n",
    "    else:\n",
    "        if(model == 'lstm'):\n",
    "            lstm = tf.nn.rnn_cell.LSTMCell(n_hidden ,state_is_tuple=True)\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell([lstm]*layers)\n",
    "\n",
    "        else:\n",
    "            gru = tf.nn.rnn_cell.GRUCell(n_hidden)\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell([gru] * layers)\n",
    "    return(cell)\n",
    "\n",
    "viewers = 1000  #number of viewers\n",
    "videos =1000  #number of videos\n",
    "probab = 0.05  #probability of a viewer watching any one video\n",
    "viewerFeat = 310  #number of features describing a veiwer\n",
    "videoFeat = 300   #number of features describing a video\n",
    "contxFeat = 15 # number of contextual features\n",
    "# X  = Dictionary with viewer features as arrays\n",
    "# Y  = Dictionary with video features as arrays\n",
    "# trData = Dictionary with training data. This is histrory of viewer and video iteraction\n",
    "X ,Y ,trData = trainSamples(viewers ,videos ,probab ,viewerFeat ,videoFeat ,contxFeat)\n",
    "\n",
    "user_feat_inp = np.array([X[key] for key in sorted(X.keys())]) \n",
    "vid_feat_inp = np.array([Y[key] for key in sorted(Y.keys())])\n",
    "\n",
    "key_user = np.asarray(range(user_feat_inp.shape[0])).reshape(user_feat_inp.shape[0] ,1)\n",
    "key_vid = np.asarray(range(vid_feat_inp.shape[0])).reshape(vid_feat_inp.shape[0] ,1)\n",
    "user_feat_inp_w_key = np.concatenate((user_feat_inp ,key_user),axis = 1)\n",
    "vid_feat_inp_w_key = np.concatenate((vid_feat_inp ,key_vid),axis = 1)\n",
    "user_vid_time = trData.keys()\n",
    "\n",
    "user_feat_inp_w_key_df = pd.DataFrame(user_feat_inp_w_key) \n",
    "vid_feat_inp_w_key_df = pd.DataFrame(vid_feat_inp_w_key)\n",
    "user_vid_time_df = pd.DataFrame(user_vid_time)\n",
    "\n",
    "rr = user_vid_time_df.sort_values([0 ,2]).reset_index()\n",
    "\n",
    "h = pd.get_dummies(rr[1],prefix = 'vid_')\n",
    "y_tr_p_w = pd.concat([rr.reset_index(drop = True), h], axis = 1)\n",
    "y_tr_p_w.rename(columns = {0: 'user'}, inplace = True)\n",
    "y_tr_p_w.rename(columns = {1: 'mov'}, inplace = True)\n",
    "y_tr_p_w.rename(columns = {2: 'tim'}, inplace = True)\n",
    "y_tr_p_w['desired'] = np.argmax(np.array(y_tr_p_w.iloc[:,4:]) ,1)\n",
    "\n",
    "max_watch = (y_tr_p_w.groupby('user',axis = 0).sum().iloc[:\n",
    "                                ,np.r_[4:(len(y_tr_p_w.columns)-2)]].sum(axis=1)).reset_index()\n",
    "not_to_sample = (y_tr_p_w.groupby('user',axis = 0).sum().reset_index()).iloc[:\n",
    "                                 ,np.r_[0 ,4:(len(y_tr_p_w.columns)-1)]]\n",
    "to_sample = (not_to_sample.iloc[:,1:].apply(lambda y:np.where(y == 0),axis = 1)).reset_index(name = 'to_samp')\n",
    "popularity = ((y_tr_p_w).sum(axis = 0)).iloc[4:-1]\n",
    "\n",
    "\n",
    "vid_feat_inp_w_key_df.rename(columns = {300: 'mov_id'}, inplace = True)\n",
    "user_feat_inp_w_key_df.rename(columns = {310: 'user_id'}, inplace = True)\n",
    "user_vid_time_df.rename(columns = {0: 'user_id'}, inplace = True)\n",
    "user_vid_time_df.rename(columns = {1: 'mov_id'}, inplace = True)\n",
    "#user_w_vid_tim_and_feat = user_vid_time_df.merge(user_feat_inp_w_key_df \n",
    "#                                                 ,how = 'inner',on = 'user_id', sort = False)\n",
    "user_vid_time_vidfeat = user_vid_time_df.merge(vid_feat_inp_w_key_df \n",
    "                                                             ,how = 'inner' ,on='mov_id' ,sort = False)\n",
    "user_vid_time_vidfeat.rename(columns={'2_x': 'time_watch'} ,inplace=True)\n",
    "user_vid_time_vidfeat_sorted = user_vid_time_vidfeat.sort_values(['user_id', 'time_watch']) \n",
    "cols_to_del = [user_vid_time_vidfeat_sorted.columns.get_loc(\"user_id\")\n",
    "             ,user_vid_time_vidfeat_sorted.columns.get_loc(\"mov_id\")\n",
    "             ,user_vid_time_vidfeat_sorted.columns.get_loc(\"time_watch\")]\n",
    "\n",
    "# ### Split training and testing data\n",
    "tr_users = viewers * 0.8\n",
    "tr_y = y_tr_p_w[y_tr_p_w['user'] < tr_users]\n",
    "te_y = y_tr_p_w[~y_tr_p_w['user'].isin(tr_y['user'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reward_gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('input'):\n",
    "    inp = tf.placeholder(\"float32\" ,[batch_size ,None, num_video] ,name = 'inp') \n",
    "    disc_pred_gen_ph = tf.placeholder(\"float32\" ,[None, 1] ,name = 'inp_ph') \n",
    "    dynam_input = tf.placeholder(\"float32\" ,[batch_size ,None ,n_feature] ,name = 'dynam_input') \n",
    "    const_input = tf.placeholder(\"float32\" ,[batch_size ,n_user_feature] ,name = 'const_input') \n",
    "    y_true = tf.placeholder(\"int32\",[None] ,name = 'Input_y')\n",
    "    max_batch_length = tf.placeholder(\"float32\" ,[batch_size] ,name = 'max_leng')\n",
    "    r_c_idx = tf.placeholder(\"int32\" ,[None,2] ,name = 'r_c_idx')\n",
    "    r_c_n_idx = tf.placeholder(\"int32\" ,[None,2] ,name = 'r_c_n_idx')\n",
    "    tr_rw = tf.placeholder(\"float32\" ,[None] ,name = 'rw_rep')\n",
    "    _maxx = tf.placeholder(\"int32\" ,name = 'maxx')\n",
    "    block_mat = tf.placeholder(\"float32\",[None, None] ,name = 'block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"weights\"):\n",
    "    W_gener_co = tf.get_variable(\"variable1\", shape=[n_user_feature ,num_video]\n",
    "                                        , initializer=tf.random_normal_initializer(), dtype='float32')\n",
    "    W_gener_ou = tf.get_variable(\"variable2\" ,shape=([n_hidden ,num_video])\n",
    "                                        , initializer=tf.random_normal_initializer(), dtype='float32')\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    assert tf.get_variable_scope().reuse==True\n",
    "\n",
    "    W_gener_co_2 = tf.get_variable(\"variable1\",shape=[n_user_feature, num_video]\n",
    "                                          , dtype='float32')\n",
    "    W_gener_ou_2 = tf.get_variable(\"variable2\", shape=[n_hidden, num_video]\n",
    "                                   , dtype='float32')\n",
    "\n",
    "with tf.variable_scope(\"biases\"):\n",
    "    b_gener_co = tf.get_variable(\"variable1\", shape=[num_video]\n",
    "                                        , initializer=tf.random_normal_initializer(), dtype='float32')\n",
    "    b_gener_ou = tf.get_variable(\"variable2\", shape=[num_video]\n",
    "                                        , initializer=tf.random_normal_initializer(), dtype='float32')\n",
    "\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    assert tf.get_variable_scope().reuse==True\n",
    "    b_gener_co_2 = tf.get_variable(\"variable1\",shape=[num_video]\n",
    "                                          , dtype='float32')\n",
    "    b_gener_ou_2 = tf.get_variable(\"variable2\",shape=[num_video]\n",
    "                                          , dtype='float32')\n",
    "\n",
    "with tf.variable_scope(\"biases_disc\"):\n",
    "    b_disc_co = tf.get_variable(\"variable1\", shape=[n_user_feature]\n",
    "                                        , initializer=tf.random_normal_initializer(), dtype='float32',)\n",
    "    b_disc_ou_1 = tf.get_variable(\"variable2\", shape=[n_user_feature]\n",
    "                                        , initializer=tf.random_normal_initializer(), dtype='float32')\n",
    "    b_disc_ou_2 = tf.get_variable(\"variable3\", shape=[1]\n",
    "                                        , initializer=tf.random_normal_initializer(), dtype='float32')\n",
    "\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    assert tf.get_variable_scope().reuse==True\n",
    "    b_disc_co_2 = tf.get_variable(\"variable1\",shape=[n_user_feature]\n",
    "                                          , dtype='float32')\n",
    "\n",
    "    b_disc_ou_1_2 = tf.get_variable(\"variable2\",shape=[n_user_feature]\n",
    "                                          , dtype='float32')\n",
    "    b_disc_ou_2_2 = tf.get_variable(\"variable3\", shape=[1]\n",
    "                                        , initializer=tf.random_normal_initializer(), dtype='float32')\n",
    "    print(b_disc_co_2==b_disc_co)\n",
    "\n",
    "with tf.variable_scope(\"weights_disc\"):\n",
    "    W_disc_co = tf.get_variable(\"variable1\", shape=[n_user_feature ,n_user_feature]\n",
    "                                        , initializer=tf.random_normal_initializer(), dtype='float32')\n",
    "    W_disc_ou = tf.get_variable(\"variable2\" ,shape=([n_hidden ,n_user_feature])\n",
    "                                        , initializer=tf.random_normal_initializer(), dtype='float32')\n",
    "    W_disc_ou_2 = tf.get_variable(\"variable3\" ,shape=([n_user_feature ,1])\n",
    "                                        , initializer=tf.random_normal_initializer(), dtype='float32')\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    assert tf.get_variable_scope().reuse==True\n",
    "\n",
    "    W_disc_co_2 = tf.get_variable(\"variable1\",shape=[n_user_feature ,n_user_feature]\n",
    "                                          , dtype='float32')\n",
    "    W_disc_ou_1_2 = tf.get_variable(\"variable2\", shape=[n_hidden ,n_user_feature]\n",
    "                                   , dtype='float32')\n",
    "    W_disc_ou_2_2 = tf.get_variable(\"variable3\", shape=[n_user_feature ,1]\n",
    "                                   , dtype='float32')\n",
    "    print(W_disc_ou==W_disc_ou_1_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_train_gener(dynam_input, const_input, max_batch_length\n",
    "        ,W_gener_co ,W_gener_ou ,b_gener_co, b_gener_ou ,model, r_c_idx, r_c_n_idx):\n",
    "    with tf.variable_scope('bb') as scope:\n",
    "        lstm_cell = rnn_model(model ,n_hidden ,layers)\n",
    "        outputs ,states = tf.nn.dynamic_rnn(lstm_cell ,inputs = dynam_input\n",
    "                             ,dtype = tf.float32 ,sequence_length = max_batch_length)\n",
    "\n",
    "    out_shaped = tf.reshape(outputs ,[-1 ,n_hidden])\n",
    "    lay_2 = tf.reshape(tf.reshape(tf.matmul(out_shaped ,W_gener_ou) + b_gener_ou\n",
    "               ,[-1,batch_size,num_video]) + (tf.matmul(const_input,\n",
    "                                                        W_gener_co) + b_gener_co), [-1,num_video]) \n",
    "    \n",
    "    p_w_d = tf.gather_nd(params = lay_2 ,indices = r_c_idx)\n",
    "    p_w_neg = tf.gather_nd(params = lay_2 ,indices = r_c_n_idx)    \n",
    "    return (lay_2 ,p_w_d ,p_w_neg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce_disc(inp, const_input, max_batch_length, W_disc_co, W_disc_ou, W_disc_ou_2, b_disc_co\n",
    "                   , b_disc_ou_1, b_disc_ou_2, model):\n",
    "    try:\n",
    "        with tf.variable_scope('aa') as scope:\n",
    "            lstm_cell = rnn_model(model ,n_hidden ,layers)\n",
    "            outputs ,states = tf.nn.dynamic_rnn(lstm_cell ,inputs = inp\n",
    "                                 ,dtype = tf.float32 ,sequence_length = max_batch_length)\n",
    "    except:\n",
    "        with tf.variable_scope('dd') as scope:\n",
    "            lstm_cell = rnn_model(model ,n_hidden ,layers)\n",
    "            outputs ,states = tf.nn.dynamic_rnn(lstm_cell ,inputs = inp\n",
    "                                 ,dtype = tf.float32 ,sequence_length = max_batch_length)\n",
    "   \n",
    "    out_shaped = tf.reshape(outputs ,[-1 ,n_hidden])\n",
    "    print (out_shaped.get_shape())\n",
    "    \n",
    "    lay_2 = tf.reshape(tf.reshape(tf.matmul(out_shaped ,W_disc_ou) + b_disc_ou_1, [-1, batch_size, n_user_feature]) \n",
    "                       + (tf.matmul(const_input, W_disc_co) + b_disc_co), [-1,n_user_feature])\n",
    "    \n",
    "    print(lay_2.get_shape())\n",
    "    \n",
    "    lay_3 = tf.matmul(lay_2 ,W_disc_ou_2) + b_disc_ou_2\n",
    "    print(lay_3.get_shape())\n",
    "    \n",
    "    return (lay_3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce_gener(inp, const_input, max_batch_length,\n",
    "        W_gener_co_2, W_gener_ou_2, b_gener_co_2, b_gener_ou_2, model, _maxx):\n",
    "    lstm_cell = rnn_model(model ,n_hidden ,layers)\n",
    "\n",
    "    outputs ,states = tf.nn.dynamic_rnn(lstm_cell ,inputs = dynam_input\n",
    "                             ,dtype = tf.float32 ,sequence_length = max_batch_length)\n",
    "\n",
    "    with tf.variable_scope('cc') as scope:\n",
    "        lstm_cell = rnn_model(model ,n_hidden ,layers)\n",
    "        outputs ,states = tf.nn.dynamic_rnn(lstm_cell ,inputs = dynam_input\n",
    "                             ,dtype = tf.float32 ,sequence_length = max_batch_length)\n",
    "\n",
    "   \n",
    "    out_shaped = tf.reshape(outputs ,[-1 ,n_hidden])\n",
    "    lay_2 = tf.reshape(tf.reshape(tf.matmul(out_shaped ,W_gener_ou_2) + b_gener_ou_2\n",
    "               ,[-1,batch_size,num_video]) + (tf.matmul(const_input,\n",
    "                                                        W_gener_co_2) + b_gener_co_2), [-1,num_video]) \n",
    "    \n",
    "   \n",
    "    lay_3 = tf.nn.softmax(lay_2)\n",
    "    lay_4 = tf.reduce_max(lay_3, 1)\n",
    "    return (lay_3, lay_4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gener, y_pred_gen_max = reinforce_gener(inp, const_input, max_batch_length,\n",
    "        W_gener_co_2, W_gener_ou_2, b_gener_co_2, b_gener_ou_2, model, _maxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, p_w_d, p_w_neg = pre_train_gener(dynam_input, const_input \n",
    "        ,max_batch_length\n",
    "        ,W_gener_co, W_gener_ou, b_gener_co, b_gener_ou, model, r_c_idx, r_c_n_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32)\n",
      "(?, 310)\n",
      "(?, 1)\n"
     ]
    }
   ],
   "source": [
    " disc_pred_gen = reinforce_disc(inp, const_input, max_batch_length, W_disc_co, W_disc_ou, W_disc_ou_2, b_disc_co\n",
    "                   , b_disc_ou_1, b_disc_ou_2, model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32)\n",
      "(?, 310)\n",
      "(?, 1)\n"
     ]
    }
   ],
   "source": [
    "disc_pred_real = reinforce_disc(inp, const_input, max_batch_length, W_disc_co_2, W_disc_ou_1_2, W_disc_ou_2_2\n",
    "                                , b_disc_co_2, b_disc_ou_1_2, b_disc_ou_2_2, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "decays = tf.tile(tf.exp(tf.log(reward_gamma) * tf.to_float(tf.range((_maxx)))), [batch_size])\n",
    "prob_disc_gen = tf.sigmoid(disc_pred_gen_ph)\n",
    "rewards = tf.matmul(block_mat, tf.reshape(tf.reshape(tf.reshape(decays,[-1,1]) * prob_disc_gen, [-1]), [-1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('pair_wise'):\n",
    "    loss_p_w = -tf.reduce_mean(tf.multiply(tf.log(tf.sigmoid(tf.subtract(p_w_d ,p_w_neg))) ,tr_rw))\n",
    "    regularizer = tf.nn.l2_loss(tf.abs(W_gener_ou_2)) \n",
    "    p_wise = tf.reduce_mean(loss_p_w + beta * regularizer)\n",
    "\n",
    "with tf.name_scope('loss_gen'):\n",
    "    g_loss = -tf.reduce_mean(tf.log(y_pred_gen_max) * rewards)\n",
    "\n",
    "with tf.name_scope('loss_d_gen'):\n",
    "       loss_d_gen = tf.reduce_mean(tf.multiply(\n",
    "         tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "             logits = disc_pred_gen, labels = tf.zeros([tf.shape(disc_pred_gen)[0], 1])), tr_rw))\n",
    "\n",
    "with tf.name_scope('loss_d_real'):\n",
    "       loss_d_real = tf.reduce_mean(tf.multiply(\n",
    "         tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "             logits = disc_pred_real, labels = tf.ones([tf.shape(disc_pred_real)[0], 1])), tr_rw))\n",
    "\n",
    "with tf.name_scope('train_d_real'):    \n",
    "    optimizer_d_r = tf.train.GradientDescentOptimizer(learning_rate = lr_rat).minimize(loss_d_real)\n",
    "\n",
    "with tf.name_scope('train_d_gen'):    \n",
    "    optimizer_d_gen = tf.train.GradientDescentOptimizer(learning_rate = lr_rat).minimize(loss_d_gen) \n",
    "\n",
    "with tf.name_scope('train_gen'):    \n",
    "    optimizer_gen = tf.train.GradientDescentOptimizer(learning_rate = lr_rat).minimize(g_loss)\n",
    "    \n",
    "with tf.name_scope('pre_train_gen'):    \n",
    "    optimizer_gen_pre = tf.train.AdamOptimizer(learning_rate = lr_rat).minimize(p_wise)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session() \n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_steps = 1\n",
    "d_steps = 1\n",
    "proportion_supervised = 0.5\n",
    "proportion_generated = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onivron/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/onivron/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/onivron/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(int(n_opt_epoch)):\n",
    "    for batch_n in range(int(n_users / batch_size)):\n",
    "        rw_to_chose = np.random.choice(int(tr_users) ,128 ,replace = False)\n",
    "        const_tr = (sort_pd_df_by_ext_vec(user_feat_inp_w_key_df, rw_to_chose,\n",
    "                                          cols = ['user_id'])).drop(['user_id'], axis = 1)\n",
    "        x_tr = sort_pd_df_by_ext_vec(user_vid_time_vidfeat_sorted,\n",
    "                                      rw_to_chose, cols = ['user_id','time_watch'])\n",
    "        d_real_inp = sort_pd_df_by_ext_vec(y_tr_p_w, rw_to_chose, cols = ['user','tim'])\n",
    "        y_batch = sort_pd_df_by_ext_vec(tr_y ,rw_to_chose,cols = ['user','tim'])\n",
    "        y_batch_p_w_idx_arr = np.array(y_batch['desired'].reset_index())\n",
    "        to_sample_sess = sort_pd_df_by_ext_vec(df = to_sample,ext_sor_vec = rw_to_chose,cols = ['index'])\n",
    "        length_vec = sort_pd_df_by_ext_vec(df = max_watch ,ext_sor_vec = rw_to_chose ,cols = ['user'])      \n",
    "        length_max = max(length_vec.iloc[:,1])\n",
    "        str_idx = algeb_geom_series(0 ,start = 0 ,jump = length_max ,length = batch_size)\n",
    "        end_idx = np.append(length_max ,length_vec.iloc[1:,1:] + str_idx[1:].reshape(batch_size-1,-1))\n",
    "        app_range = range_bet_col_t_col_n_append(str_idx, end_idx)\n",
    "        ind_r_t_pick = (np.isin(range(int(length_max * batch_size)) ,app_range)).astype(int)\n",
    "        rep_ind = np.repeat(ind_r_t_pick,10)\n",
    "        trial_size = (batch_size * length_max, n_feature)\n",
    "        trial_size_d = (batch_size * length_max, num_video)\n",
    "        zero_array_x = np.zeros(trial_size); zero_array_y = np.zeros(batch_size * length_max); zero_array_d_r =np.zeros(trial_size_d) \n",
    "        x_tr, y_tr = np_pad_tr_x_n_tr_y(x_tr.iloc[:,3:], y_batch, batch_size, str_idx.astype(int),\n",
    "                                        zero_array_x, zero_array_y, length_vec.iloc[:,1].astype(int))\n",
    "        d_real_inp = np_pad_tr_x(d_real_inp.iloc[:,4:-1], batch_size, str_idx.astype(int),\n",
    "                                        zero_array_d_r, length_vec.iloc[:,1].astype(int))\n",
    "        neg_sample, idx_p_w, rep_seq = negtv_sample(to_sample_sess ,popularity\n",
    "                                                    ,batch_size ,length_max ,num_to_sample = 10)\n",
    "        idx = np.repeat(y_tr,10)       \n",
    "        idx_pp = np.concatenate((rep_seq ,idx.reshape(-1,1)),axis = 1)\n",
    "        block_tri = block_tri_mat(length_max, batch_size)\n",
    "        \n",
    "        \n",
    "        if (epoch < 1): # pre_train \n",
    "            _, g_p_w, g_pred = sess.run([optimizer_gen_pre, p_wise, y_pred], feed_dict = \n",
    "                                         {dynam_input: x_tr.reshape(batch_size,-1,n_feature)\n",
    "                                                  ,const_input: const_tr \n",
    "                                                  ,tr_rw: rep_ind\n",
    "                                                  ,r_c_idx: idx_pp\n",
    "                                                  ,r_c_n_idx: idx_p_w\n",
    "                                                  ,max_batch_length: length_vec.iloc[:,1]})\n",
    "        else:\n",
    "            for _ in range(g_steps):\n",
    "                if np.random.random() < proportion_supervised:\n",
    "                    _, _g_p_w, _g_pred = sess.run([optimizer_gen_pre, p_wise, y_pred], feed_dict = \n",
    "                                         {dynam_input: x_tr.reshape(batch_size,-1,n_feature)\n",
    "                                                  ,const_input: const_tr \n",
    "                                                  ,tr_rw: rep_ind\n",
    "                                                  ,r_c_idx: idx_pp\n",
    "                                                  ,r_c_n_idx: idx_p_w\n",
    "                                                  ,y_true: y_tr\n",
    "                                                  ,max_batch_length: length_vec.iloc[:,1]})\n",
    "                else: \n",
    "                    gen_xx = sess.run(y_pred_gener, feed_dict = {\n",
    "                            dynam_input: x_tr.reshape(batch_size, -1, n_feature)\n",
    "                                                  ,const_input: const_tr \n",
    "                                                  ,max_batch_length: length_vec.iloc[:,1]\n",
    "                                                })\n",
    "                    \n",
    "                    d_xx = sess.run(disc_pred_gen, feed_dict = {inp: gen_xx.reshape(batch_size, -1, num_video)\n",
    "                            ,const_input: const_tr\n",
    "                            ,max_batch_length: length_vec.iloc[:,1]})\n",
    "                    \n",
    "                    \n",
    "                    _, g_l, unsupervised_gen_x, _rewards = sess.run([optimizer_gen, g_loss, y_pred_gener, rewards]\n",
    "                                                                    , feed_dict = \n",
    "                                         {dynam_input: x_tr.reshape(batch_size, -1, n_feature),\n",
    "                                                  _maxx: length_max, \n",
    "                                                  disc_pred_gen_ph: d_xx,\n",
    "                                                  block_mat: block_tri \n",
    "                                                  ,const_input: const_tr \n",
    "                                                  ,max_batch_length: length_vec.iloc[:, 1]})\n",
    "        if ((epoch >= 1) & (epoch <= 2)) :\n",
    "             _, _disc_pred_real_pre, d_loss_pre = sess.run([optimizer_d_r, disc_pred_real, loss_d_real], feed_dict = \n",
    "                                                         {inp: d_real_inp.reshape(batch_size, -1, num_video)\n",
    "                                                          ,const_input: const_tr\n",
    "                                                          ,max_batch_length: length_vec.iloc[:,1]\n",
    "                                                          ,tr_rw: rep_ind \n",
    "                            \n",
    "                        })\n",
    "            \n",
    "        if (epoch > 2):\n",
    "            for _ in range(d_steps):\n",
    "                if np.random.random() < proportion_generated:\n",
    "                    _gen_xx = sess.run(y_pred_gener, feed_dict = {\n",
    "                            dynam_input: x_tr.reshape(batch_size, -1, n_feature)\n",
    "                                                  ,const_input: const_tr \n",
    "                                                  ,max_batch_length: length_vec.iloc[:,1]\n",
    "                                    })\n",
    "                    _, _disc_pred_gen, d_loss = sess.run([optimizer_d_gen, disc_pred_gen, loss_d_gen], feed_dict = \n",
    "                                                   {inp: _gen_xx.reshape(batch_size, -1, num_video)\n",
    "                                                    ,const_input: const_tr\n",
    "                                                    ,max_batch_length: length_vec.iloc[:,1]})\n",
    "                else:\n",
    "                    _, _disc_pred_real, _d_loss = sess.run([optimizer_d_r, disc_pred_real, loss_d_real], feed_dict = \n",
    "                                                         {inp: d_real_inp.reshape(batch_size, -1, num_video)\n",
    "                                                          ,const_input: const_tr\n",
    "                                                          ,max_batch_length: length_vec.iloc[:,1]\n",
    "                                                          ,tr_rw: rep_ind \n",
    "                            \n",
    "                        })\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
