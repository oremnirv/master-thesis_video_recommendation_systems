{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><u>RNN thesis</u></h1>\n",
    "<h4>Omer Nivron</h4>\n",
    "15098443"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Package loading</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys as sys\n",
    "import random as rd\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "from __future__ import division\n",
    "from tensorflow.python.ops import array_ops, control_flow_ops\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import boto3\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import cStringIO\n",
    "! sudo pip install s3fs\n",
    "import s3fs\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def np_pad_tr_x(x_tr, batch_size, str_idx, zero_array_x, length_vec):\n",
    "    start = 0\n",
    "    for i in range(batch_size):\n",
    "        if i > 0:\n",
    "            end = end + length_vec[i] \n",
    "        else:\n",
    "            end = length_vec[0]\n",
    "        zero_array_x[ int(str_idx[i]) : int(str_idx[i] + length_vec[i]) ] = x_tr[ start : end ] \n",
    "        start = end\n",
    "\n",
    "    return(zero_array_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def range_bet_col_t_col_n_append(col_1 ,col_2):\n",
    "    app_ranges=[]\n",
    "    for i in range(col_1.shape[0]):\n",
    "        single_range = range((col_1[i]).astype(int), (col_2[i]).astype(int))\n",
    "        app_ranges = np.append(app_ranges, single_range)\n",
    "    return(app_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def algeb_geom_series(mode ,start ,jump ,length):\n",
    "    u = np.empty((length,))\n",
    "    u[0] = start\n",
    "    u[1:] = jump\n",
    "    if (mode == 0):\n",
    "        series=np.cumsum(u)\n",
    "    if (mode == 1):\n",
    "        series=np.cumprod(u)\n",
    "    return(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_pd_df_by_ext_vec(df,ext_sor_vec, cols):\n",
    "    df_s = df[((df[cols[0]]).astype(int)).isin(ext_sor_vec)] #\n",
    "    df_s['sort_cat'] = pd.Categorical(df_s[cols[0]],categories = ext_sor_vec,ordered = True)\n",
    "    if len(cols) > 1:\n",
    "        df_s.sort_values(['sort_cat',cols[1]] ,inplace = True)\n",
    "    \n",
    "    else:\n",
    "        df_s.sort_values(['sort_cat'],inplace = True) \n",
    "    \n",
    "    df_s.reset_index(inplace = True)\n",
    "    df_ = df_s.drop(['sort_cat','index'] ,axis = 1)\n",
    "    \n",
    "    return(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_model(model ,n_hidden ,layers):\n",
    "    if (layers == 1):\n",
    "        if(model == 'lstm'):\n",
    "            cell = tf.nn.rnn_cell.LSTMCell(n_hidden ,state_is_tuple=True)\n",
    "        else:\n",
    "            cell = tf.nn.rnn_cell.GRUCell(n_hidden)   \n",
    "    else:\n",
    "        if(model == 'lstm'):\n",
    "            lstm = tf.nn.rnn_cell.LSTMCell(n_hidden ,state_is_tuple=True)\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell([lstm]*layers)\n",
    "\n",
    "        else:\n",
    "            gru = tf.nn.rnn_cell.GRUCell(n_hidden)\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell([gru] * layers)\n",
    "    return(cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Fake Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainSamples(viewers,videos,probab,viewerFeat,videoFeat,contxFeat):\n",
    "    trData = {} # trData = Dictionary with training data. This is histrory of viewer and video iteraction\n",
    "    X = {} # X  = Dictionary with viewer features as arrays\n",
    "    Y ={} # Y  = Dictionary with video features as arrays\n",
    "    for i in range(viewers):\n",
    "        X[i] = np.random.rand(viewerFeat)\n",
    "        a = 0 # timing of the video for a particular user,... \n",
    "                #to give the order in which the videos have been watched\n",
    "        for j in range(videos):\n",
    "            if int(np.random.binomial(1,probab ,1)[0]):\n",
    "                trData[(i,j,a)] = np.random.rand(contxFeat)\n",
    "                a+=1 # when a video is watched, we increase the value of a by 1 \n",
    "            if i==0:\n",
    "                Y[j] = np.random.rand(videoFeat)\n",
    "    return X,Y,trData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "viewers = 1000  #number of viewers\n",
    "videos =1000  #number of videos\n",
    "probab = 0.05  #probability of a viewer watching any one video\n",
    "viewerFeat = 310  #number of features describing a veiwer\n",
    "videoFeat = 300   #number of features describing a video\n",
    "contxFeat = 15 # number of contextual features\n",
    "# X  = Dictionary with viewer features as arrays\n",
    "# Y  = Dictionary with video features as arrays\n",
    "# trData = Dictionary with training data. This is histrory of viewer and video iteraction\n",
    "X ,Y ,trData = trainSamples(viewers ,videos ,probab ,viewerFeat ,videoFeat ,contxFeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform a dict to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_feat_inp = np.array([X[key] for key in sorted(X.keys())]) \n",
    "vid_feat_inp = np.array([Y[key] for key in sorted(Y.keys())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a column key specifying user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_user = np.asarray(range(user_feat_inp.shape[0])).reshape(user_feat_inp.shape[0] ,1)\n",
    "key_vid = np.asarray(range(vid_feat_inp.shape[0])).reshape(vid_feat_inp.shape[0] ,1)\n",
    "user_feat_inp_w_key = np.concatenate((user_feat_inp ,key_user),axis = 1)\n",
    "vid_feat_inp_w_key = np.concatenate((vid_feat_inp ,key_vid),axis = 1)\n",
    "user_vid_time = trData.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform numpy to pandas df in order to use easy merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_feat_inp_w_key_df = pd.DataFrame(user_feat_inp_w_key) \n",
    "vid_feat_inp_w_key_df = pd.DataFrame(vid_feat_inp_w_key)\n",
    "user_vid_time_df = pd.DataFrame(user_vid_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct true_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort values by user and time watched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rr = user_vid_time_df.sort_values([0 ,2]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get categorical column for each video with 1 where watched and 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h = pd.get_dummies(rr[1],prefix = 'vid_')\n",
    "y_tr_p_w = pd.concat([rr.reset_index(drop = True), h], axis = 1)\n",
    "y_tr_p_w.rename(columns = {0: 'user'}, inplace = True)\n",
    "y_tr_p_w.rename(columns = {1: 'mov'}, inplace = True)\n",
    "y_tr_p_w.rename(columns = {2: 'rank'}, inplace = True)\n",
    "y_tr_p_w['desired'] = np.argmax(np.array(y_tr_p_w.iloc[:,4:]) ,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We get for each user one row with 1's in the positions watched incl. user column(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_watch = (y_tr_p_w.groupby('user',axis = 0).sum().iloc[:\n",
    "                                ,int(np.array(np.where(y_tr_p_w.columns=='rank'))):-1].sum(axis=1)).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge two pandas df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vid_feat_inp_w_key_df.rename(columns = {300: 'mov_id'}, inplace = True)\n",
    "user_feat_inp_w_key_df.rename(columns = {310: 'user_id'}, inplace = True)\n",
    "user_vid_time_df.rename(columns = {0: 'user_id'}, inplace = True)\n",
    "user_vid_time_df.rename(columns = {1: 'mov_id'}, inplace = True)\n",
    "user_w_vid_tim_and_feat = user_vid_time_df.merge(user_feat_inp_w_key_df \n",
    "                                                 ,how = 'inner',on = 'user_id', sort = False)\n",
    "user_vid_time_vidfeat_usefit = user_w_vid_tim_and_feat.merge(vid_feat_inp_w_key_df \n",
    "                                                             ,how = 'inner' ,on='mov_id' ,sort = False)\n",
    "user_vid_time_vidfeat_usefit.rename(columns={'2_x': 'time_watch'} ,inplace=True)\n",
    "user_vid_time_vidfeat_usefit_sorted = user_vid_time_vidfeat_usefit.sort_values(['user_id', 'time_watch']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_vid_time_vidfeat_usefit_sorted_dis = user_vid_time_vidfeat_usefit_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_users = viewers * 0.8\n",
    "tr_y = y_tr_p_w[y_tr_p_w['user'] < tr_users]\n",
    "te_y = y_tr_p_w[~y_tr_p_w['user'].isin(tr_y['user'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Configurations</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LSTM with X units</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_opt_epoch = 1000\n",
    "model = 'lstm'\n",
    "layers = 1\n",
    "n_samples = 2000\n",
    "top_k = 15\n",
    "top_l = 10\n",
    "top_r = 5\n",
    "fixed = 10\n",
    "top_all = 2000\n",
    "n_users = num_users\n",
    "te_users = int(np.ceil(num_users*0.2))\n",
    "n_feature = num_user_feat + num_video_feat + contex_feat #623\n",
    "n_feature_dis = num_video_feat\n",
    "lr_rat = 0.001\n",
    "num_video = h.shape[1]\n",
    "beta = 0.01\n",
    "\n",
    "if tr_users > 500:\n",
    "    batch_size = 500\n",
    "else:\n",
    "    batch_size  = tr_users\n",
    "        \n",
    "n_hidden = 64 # hidden layer num of features\n",
    "n_hidden_dis = 64 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard saving paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! mkdir -p ./reco_rnn/tensor_plot/dis_xent/\n",
    "! mkdir -p ./reco_rnn/dis_xent/\n",
    "#logs_path = \"/Users/onivron/Desktop/reco_rnn/tensor_plot/p_w/\"\n",
    "logs_path = \"./reco_rnn/tensor_plot/dis_xent/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Var creation</h3>\n",
    "We build data placeholders & variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('input'):\n",
    "    dislike_inp = tf.placeholder(\"float32\" ,[batch_size ,fixed, n_feature_dis] ,name = 'dislike_inp') \n",
    "    dynam_input = tf.placeholder(\"float32\" ,[batch_size ,None, n_feature] ,name = 'dynam_input') \n",
    "    y_true = tf.placeholder(\"int32\",[None, 1] ,name = 'Input_y')\n",
    "    max_batch_length = tf.placeholder(\"float32\" ,[batch_size] ,name = 'max_leng')\n",
    "    tr_rw = tf.placeholder(\"int32\" ,[None, 1] ,name = '_rw')\n",
    "    tr_rw_n_desired_rep = tf.placeholder(\"int32\" ,[None, 2] ,name = 'rw_rep')\n",
    "    tr_rw_non_rep = tf.placeholder(\"int32\" ,[None, 2] ,name = 'non_rep')\n",
    "    \n",
    "with tf.name_scope(\"weights\"):\n",
    "    W = {'out_1': tf.Variable(tf.random_normal([1500 ,num_video])  ,name = 'out_1'),\n",
    "        'out_2': tf.Variable(tf.random_normal([n_hidden ,num_video]) ,name = 'w_2') , \n",
    "        'dis': tf.Variable(tf.random_normal([fixed * n_feature_dis, 2000])  ,name = 'dis'),\n",
    "        'dis_2': tf.Variable(tf.random_normal([2000 ,1500])  ,name = 'dis_2')}\n",
    "    \n",
    "with tf.name_scope(\"biases\"):\n",
    "    b = {'out_1': tf.Variable(tf.random_normal([num_video]) ,name = 'b_1'),\n",
    "        'out_2': tf.Variable(tf.random_normal([num_video]) ,name = 'b_2') , \n",
    "        'dis': tf.Variable(tf.random_normal([2000])  ,name = 'b_dis'),\n",
    "        'dis_2': tf.Variable(tf.random_normal([1500])  ,name = 'b_dis_2')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(dynam_input, dislike_inp \n",
    "        ,max_batch_length\n",
    "         ,W ,b ,model):\n",
    "    \n",
    "    lstm_cell = rnn_model(model ,n_hidden ,layers)\n",
    "    \n",
    "    outputs ,states = tf.nn.dynamic_rnn(lstm_cell ,inputs = dynam_input\n",
    "                                     ,dtype = tf.float32 ,sequence_length = max_batch_length)\n",
    "    \n",
    "    \n",
    "    out_shaped = tf.reshape(outputs ,[-1 ,n_hidden])\n",
    "    \n",
    "    dislike_layer = (tf.matmul(tf.reshape(dislike_inp, [-1, fixed * n_feature_dis]) \n",
    "                                         , W['dis']) + b['dis']) \n",
    "    \n",
    "    dislike_layer_embeded = tf.nn.relu(tf.matmul(dislike_layer ,W['dis_2']) + b['dis_2']) \n",
    "    \n",
    "    dislike_score = tf.nn.relu(tf.matmul(dislike_layer_embeded, W['out_1']) + b['out_1']) \n",
    "    \n",
    "    like_score = tf.reshape((tf.matmul(out_shaped ,W['out_2']) + b['out_2']), [-1, batch_size ,num_video])\n",
    "    \n",
    "    layer_3 = dislike_score + like_score\n",
    "    \n",
    "    relative_like = tf.divide(like_score, layer_3)\n",
    "        \n",
    "    return (relative_like, dislike_score, like_score, layer_3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define loss, accuracy & optimizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_relative_like, _dislike_score, _like_score, y_pred  = RNN(dynam_input, dislike_inp, max_batch_length, W, b, model)\n",
    "\n",
    "with tf.name_scope('like_facotr'):\n",
    "    like_factor = tf.gather_nd(params = tf.reshape(_relative_like, [-1, num_video]), indices = tr_rw_non_rep)\n",
    "\n",
    "relevant_y_pred = tf.reshape(tf.gather(params = tf.reshape(y_pred\n",
    "                                        ,[-1 ,num_video]), indices = tr_rw), [-1, num_video])\n",
    "\n",
    "top_vals, top_indice = tf.nn.top_k(relevant_y_pred, n_samples)\n",
    "\n",
    "\n",
    "with tf.name_scope('cross_entropy'):\n",
    "\n",
    "    xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = relevant_y_pred, \n",
    "                            labels = tf.reshape(y_true, [-1]))) \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "    regularizer_1 = tf.nn.l2_loss(tf.abs(W['out_2']))\n",
    "    regularizer_2 = tf.nn.l2_loss(tf.abs(W['out_1'])) \n",
    "    regularizer_3 = tf.nn.l2_loss(tf.abs(W['dis_2'])) \n",
    "    regularizer_4 =  tf.nn.l2_loss(tf.abs(W['dis']))\n",
    "    \n",
    "    \n",
    "    xent_regu = tf.reduce_mean(xent + beta * regularizer_1 \n",
    "                               + beta * regularizer_2\n",
    "                               + beta * regularizer_3\n",
    "                               + beta * regularizer_4\n",
    "                              )\n",
    "    \n",
    "    \n",
    "with tf.name_scope('Accuracy_top_r'):\n",
    "    top_15_indx_r = tf.slice(top_indice,[0,0],[-1, top_r])\n",
    "    to_bool_r = tf.reduce_sum(tf.cast(tf.equal(y_true, top_15_indx_r), tf.float32), 1)\n",
    "    accuracy_r = tf.reduce_mean(to_bool_r)   \n",
    "    \n",
    "with tf.name_scope('Accuracy_top_l'):\n",
    "    top_15_indx_l = tf.slice(top_indice,[0,0],[-1, top_l])\n",
    "    to_bool_l = tf.reduce_sum(tf.cast(tf.equal(y_true, top_15_indx_l), tf.float32), 1)\n",
    "    accuracy_l = tf.reduce_mean(to_bool_l)\n",
    "    \n",
    "with tf.name_scope('Accuracy_top_k'):\n",
    "    top_15_indx_k = tf.slice(top_indice,[0,0],[-1,top_k])\n",
    "    to_bool_k = tf.reduce_sum(tf.cast(tf.equal(y_true, top_15_indx_k), tf.float32), 1)\n",
    "    accuracy_k = tf.reduce_mean(to_bool_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('train'):    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = lr_rat).minimize(xent_regu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tensorboard set-up</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.summary.scalar(\"cost\", xent_regu)\n",
    "tf.summary.scalar(\"Accuracy_top_r\", accuracy_r)\n",
    "tf.summary.scalar(\"Accuracy_top_k\", accuracy_k)\n",
    "tf.summary.scalar(\"Accuracy_top_l\", accuracy_l)\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "writer_opt = tf.summary.FileWriter(logs_path ,graph = tf.get_default_graph())\n",
    "saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training - optimization</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session() \n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./reco_rnn/dis_xent/accu\n"
     ]
    }
   ],
   "source": [
    "save_MDir = './reco_rnn/dis_xent/'\n",
    "save_model = os.path.join(save_MDir,'accu')\n",
    "sess=tf.Session() \n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver.restore(sess = sess,save_path = save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/lib/python2.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/usr/lib/python2.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/lib/python2.7/dist-packages/ipykernel_launcher.py:26: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/usr/lib/python2.7/dist-packages/ipykernel_launcher.py:35: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 0 loss: 132258.0 accuracy 15: 0.00194633\n",
      "epoch number: 0 accuracy 10: 0.00133914\n",
      "epoch number: 0 accuracy 5: 0.000615507\n",
      "epoch number: 0 loss: 131497.0 accuracy 15: 0.00212514\n",
      "epoch number: 0 accuracy 10: 0.00138093\n",
      "epoch number: 0 accuracy 5: 0.000694599\n",
      "epoch number: 0 loss: 131049.0 accuracy 15: 0.00195626\n",
      "epoch number: 0 accuracy 10: 0.00139852\n",
      "epoch number: 0 accuracy 5: 0.000707584\n",
      "epoch number: 10 loss: 121605.0 accuracy 15: 0.00319317\n",
      "epoch number: 10 accuracy 10: 0.00217566\n",
      "epoch number: 10 accuracy 5: 0.00117469\n",
      "epoch number: 10 loss: 121370.0 accuracy 15: 0.00291686\n",
      "epoch number: 10 accuracy 10: 0.00205847\n",
      "epoch number: 10 accuracy 5: 0.00115008\n",
      "epoch number: 10 loss: 121085.0 accuracy 15: 0.00320202\n",
      "epoch number: 10 accuracy 10: 0.00212362\n",
      "epoch number: 10 accuracy 5: 0.00111988\n",
      "epoch number: 20 loss: 116002.0 accuracy 15: 0.00352084\n",
      "epoch number: 20 accuracy 10: 0.00226752\n",
      "epoch number: 20 accuracy 5: 0.00125332\n",
      "epoch number: 20 loss: 115829.0 accuracy 15: 0.0035111\n",
      "epoch number: 20 accuracy 10: 0.00231037\n",
      "epoch number: 20 accuracy 5: 0.00126698\n",
      "epoch number: 20 loss: 115649.0 accuracy 15: 0.00379171\n",
      "epoch number: 20 accuracy 10: 0.00258264\n",
      "epoch number: 20 accuracy 5: 0.00140647\n",
      "epoch number: 30 loss: 111944.0 accuracy 15: 0.00372885\n",
      "epoch number: 30 accuracy 10: 0.0024914\n",
      "epoch number: 30 accuracy 5: 0.00130345\n",
      "epoch number: 30 loss: 111755.0 accuracy 15: 0.00389668\n",
      "epoch number: 30 accuracy 10: 0.00269707\n",
      "epoch number: 30 accuracy 5: 0.00135681\n",
      "epoch number: 30 loss: 111658.0 accuracy 15: 0.00381162\n",
      "epoch number: 30 accuracy 10: 0.00256313\n",
      "epoch number: 30 accuracy 5: 0.00136425\n",
      "epoch number: 40 loss: 108431.0 accuracy 15: 0.0037319\n",
      "epoch number: 40 accuracy 10: 0.00256257\n",
      "epoch number: 40 accuracy 5: 0.00125226\n",
      "epoch number: 40 loss: 108377.0 accuracy 15: 0.00387267\n",
      "epoch number: 40 accuracy 10: 0.00258178\n",
      "epoch number: 40 accuracy 5: 0.00129089\n",
      "epoch number: 40 loss: 108330.0 accuracy 15: 0.00428224\n",
      "epoch number: 40 accuracy 10: 0.00279896\n",
      "epoch number: 40 accuracy 5: 0.00131568\n",
      "epoch number: 50 loss: 105548.0 accuracy 15: 0.00399256\n",
      "epoch number: 50 accuracy 10: 0.00283397\n",
      "epoch number: 50 accuracy 5: 0.00143366\n",
      "epoch number: 50 loss: 105425.0 accuracy 15: 0.00396493\n",
      "epoch number: 50 accuracy 10: 0.0027207\n",
      "epoch number: 50 accuracy 5: 0.00126081\n",
      "epoch number: 50 loss: 105319.0 accuracy 15: 0.00388819\n",
      "epoch number: 50 accuracy 10: 0.00261994\n",
      "epoch number: 50 accuracy 5: 0.00130997\n",
      "epoch number: 190 loss: 78367.5 accuracy 15: 0.073258\n",
      "epoch number: 190 accuracy 10: 0.0635565\n",
      "epoch number: 190 accuracy 5: 0.0505043\n",
      "epoch number: 190 loss: 78319.8 accuracy 15: 0.0760665\n",
      "epoch number: 190 accuracy 10: 0.0660549\n",
      "epoch number: 190 accuracy 5: 0.0519263\n",
      "epoch number: 190 loss: 78287.5 accuracy 15: 0.0716344\n",
      "epoch number: 190 accuracy 10: 0.0624534\n",
      "epoch number: 190 accuracy 5: 0.0493402\n",
      "epoch number: 200 loss: 77095.6 accuracy 15: 0.087916\n",
      "epoch number: 200 accuracy 10: 0.0767696\n",
      "epoch number: 200 accuracy 5: 0.0613342\n",
      "epoch number: 200 loss: 77061.1 accuracy 15: 0.0825421\n",
      "epoch number: 200 accuracy 10: 0.072175\n",
      "epoch number: 200 accuracy 5: 0.057353\n",
      "epoch number: 200 loss: 77014.6 accuracy 15: 0.0919059\n",
      "epoch number: 200 accuracy 10: 0.080396\n",
      "epoch number: 200 accuracy 5: 0.0642128\n",
      "epoch number: 210 loss: 75857.9 accuracy 15: 0.0956799\n",
      "epoch number: 210 accuracy 10: 0.0840594\n",
      "epoch number: 210 accuracy 5: 0.0668655\n",
      "epoch number: 210 loss: 75818.5 accuracy 15: 0.0962418\n",
      "epoch number: 210 accuracy 10: 0.0843787\n",
      "epoch number: 210 accuracy 5: 0.0677586\n",
      "epoch number: 210 loss: 75783.7 accuracy 15: 0.093652\n",
      "epoch number: 210 accuracy 10: 0.0818113\n",
      "epoch number: 210 accuracy 5: 0.0649916\n",
      "epoch number: 220 loss: 74642.2 accuracy 15: 0.103879\n",
      "epoch number: 220 accuracy 10: 0.0914506\n",
      "epoch number: 220 accuracy 5: 0.0738902\n",
      "epoch number: 220 loss: 74600.5 accuracy 15: 0.112349\n",
      "epoch number: 220 accuracy 10: 0.0995969\n",
      "epoch number: 220 accuracy 5: 0.0801919\n",
      "epoch number: 220 loss: 74560.9 accuracy 15: 0.109208\n",
      "epoch number: 220 accuracy 10: 0.0966047\n",
      "epoch number: 220 accuracy 5: 0.0783248\n",
      "epoch number: 230 loss: 73436.5 accuracy 15: 0.1154\n",
      "epoch number: 230 accuracy 10: 0.1025\n",
      "epoch number: 230 accuracy 5: 0.0829606\n",
      "epoch number: 230 loss: 73397.7 accuracy 15: 0.117053\n",
      "epoch number: 230 accuracy 10: 0.104018\n",
      "epoch number: 230 accuracy 5: 0.0844133\n",
      "epoch number: 230 loss: 73357.1 accuracy 15: 0.112863\n",
      "epoch number: 230 accuracy 10: 0.100481\n",
      "epoch number: 230 accuracy 5: 0.0813779\n",
      "epoch number: 240 loss: 72244.8 accuracy 15: 0.119531\n",
      "epoch number: 240 accuracy 10: 0.106885\n",
      "epoch number: 240 accuracy 5: 0.0872049\n",
      "epoch number: 240 loss: 72208.5 accuracy 15: 0.123713\n",
      "epoch number: 240 accuracy 10: 0.110432\n",
      "epoch number: 240 accuracy 5: 0.0896742\n",
      "epoch number: 240 loss: 72166.5 accuracy 15: 0.118955\n",
      "epoch number: 240 accuracy 10: 0.105829\n",
      "epoch number: 240 accuracy 5: 0.0860442\n",
      "epoch number: 250 loss: 71068.5 accuracy 15: 0.130983\n",
      "epoch number: 250 accuracy 10: 0.11737\n",
      "epoch number: 250 accuracy 5: 0.0969386\n",
      "epoch number: 250 loss: 71029.7 accuracy 15: 0.127653\n",
      "epoch number: 250 accuracy 10: 0.114769\n",
      "epoch number: 250 accuracy 5: 0.0938884\n",
      "epoch number: 250 loss: 70990.6 accuracy 15: 0.129332\n",
      "epoch number: 250 accuracy 10: 0.115787\n",
      "epoch number: 250 accuracy 5: 0.0948228\n",
      "epoch number: 260 loss: 69902.2 accuracy 15: 0.133452\n",
      "epoch number: 260 accuracy 10: 0.11998\n",
      "epoch number: 260 accuracy 5: 0.0985821\n",
      "epoch number: 260 loss: 69863.3 accuracy 15: 0.130029\n",
      "epoch number: 260 accuracy 10: 0.116881\n",
      "epoch number: 260 accuracy 5: 0.0959566\n",
      "epoch number: 260 loss: 69824.8 accuracy 15: 0.13179\n",
      "epoch number: 260 accuracy 10: 0.118326\n",
      "epoch number: 260 accuracy 5: 0.0970327\n",
      "epoch number: 270 loss: 68747.5 accuracy 15: 0.14037\n",
      "epoch number: 270 accuracy 10: 0.126852\n",
      "epoch number: 270 accuracy 5: 0.104591\n",
      "epoch number: 270 loss: 68709.2 accuracy 15: 0.143319\n",
      "epoch number: 270 accuracy 10: 0.12939\n",
      "epoch number: 270 accuracy 5: 0.107158\n",
      "epoch number: 270 loss: 68670.9 accuracy 15: 0.139798\n",
      "epoch number: 270 accuracy 10: 0.126399\n",
      "epoch number: 270 accuracy 5: 0.10485\n",
      "epoch number: 420 loss: 52739.7 accuracy 15: 0.192057\n",
      "epoch number: 420 accuracy 10: 0.175261\n",
      "epoch number: 420 accuracy 5: 0.150293\n",
      "epoch number: 420 loss: 52706.7 accuracy 15: 0.202414\n",
      "epoch number: 420 accuracy 10: 0.185549\n",
      "epoch number: 420 accuracy 5: 0.159469\n",
      "epoch number: 420 loss: 52673.9 accuracy 15: 0.193609\n",
      "epoch number: 420 accuracy 10: 0.176855\n",
      "epoch number: 420 accuracy 5: 0.151527\n",
      "epoch number: 430 loss: 51757.6 accuracy 15: 0.198123\n",
      "epoch number: 430 accuracy 10: 0.180797\n",
      "epoch number: 430 accuracy 5: 0.15451\n",
      "epoch number: 430 loss: 51725.1 accuracy 15: 0.196015\n",
      "epoch number: 430 accuracy 10: 0.178799\n",
      "epoch number: 430 accuracy 5: 0.153141\n",
      "epoch number: 430 loss: 51692.6 accuracy 15: 0.205207\n",
      "epoch number: 430 accuracy 10: 0.187654\n",
      "epoch number: 430 accuracy 5: 0.160987\n",
      "epoch number: 440 loss: 50786.4 accuracy 15: 0.20313\n",
      "epoch number: 440 accuracy 10: 0.185899\n",
      "epoch number: 440 accuracy 5: 0.159116\n",
      "epoch number: 440 loss: 50754.2 accuracy 15: 0.197225\n",
      "epoch number: 440 accuracy 10: 0.180268\n",
      "epoch number: 440 accuracy 5: 0.153951\n",
      "epoch number: 440 loss: 50722.0 accuracy 15: 0.191228\n",
      "epoch number: 440 accuracy 10: 0.174451\n",
      "epoch number: 440 accuracy 5: 0.149293\n",
      "epoch number: 450 loss: 49825.5 accuracy 15: 0.200353\n",
      "epoch number: 450 accuracy 10: 0.182824\n",
      "epoch number: 450 accuracy 5: 0.15653\n",
      "epoch number: 450 loss: 49793.5 accuracy 15: 0.207932\n",
      "epoch number: 450 accuracy 10: 0.189949\n",
      "epoch number: 450 accuracy 5: 0.162779\n",
      "epoch number: 450 loss: 49761.7 accuracy 15: 0.200328\n",
      "epoch number: 450 accuracy 10: 0.182949\n",
      "epoch number: 450 accuracy 5: 0.156135\n",
      "epoch number: 460 loss: 48874.8 accuracy 15: 0.204866\n",
      "epoch number: 460 accuracy 10: 0.187195\n",
      "epoch number: 460 accuracy 5: 0.160021\n",
      "epoch number: 460 loss: 48843.4 accuracy 15: 0.201471\n",
      "epoch number: 460 accuracy 10: 0.184129\n",
      "epoch number: 460 accuracy 5: 0.157414\n",
      "epoch number: 460 loss: 48811.9 accuracy 15: 0.202949\n",
      "epoch number: 460 accuracy 10: 0.185279\n",
      "epoch number: 460 accuracy 5: 0.158063\n",
      "epoch number: 470 loss: 47935.1 accuracy 15: 0.200444\n",
      "epoch number: 470 accuracy 10: 0.183159\n",
      "epoch number: 470 accuracy 5: 0.156252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 470 loss: 47903.8 accuracy 15: 0.204441\n",
      "epoch number: 470 accuracy 10: 0.186763\n",
      "epoch number: 470 accuracy 5: 0.15871\n",
      "epoch number: 470 loss: 47872.8 accuracy 15: 0.196055\n",
      "epoch number: 470 accuracy 10: 0.178692\n",
      "epoch number: 470 accuracy 5: 0.151732\n",
      "epoch number: 480 loss: 47005.5 accuracy 15: 0.215866\n",
      "epoch number: 480 accuracy 10: 0.196923\n",
      "epoch number: 480 accuracy 5: 0.168508\n",
      "epoch number: 480 loss: 46974.8 accuracy 15: 0.207636\n",
      "epoch number: 480 accuracy 10: 0.189898\n",
      "epoch number: 480 accuracy 5: 0.162328\n",
      "epoch number: 480 loss: 46944.0 accuracy 15: 0.199036\n",
      "epoch number: 480 accuracy 10: 0.18155\n",
      "epoch number: 480 accuracy 5: 0.154394\n",
      "epoch number: 490 loss: 46086.8 accuracy 15: 0.210004\n",
      "epoch number: 490 accuracy 10: 0.191815\n",
      "epoch number: 490 accuracy 5: 0.163634\n",
      "epoch number: 490 loss: 46056.3 accuracy 15: 0.203843\n",
      "epoch number: 490 accuracy 10: 0.185326\n",
      "epoch number: 490 accuracy 5: 0.157634\n",
      "epoch number: 490 loss: 46025.9 accuracy 15: 0.204593\n",
      "epoch number: 490 accuracy 10: 0.18627\n",
      "epoch number: 490 accuracy 5: 0.158256\n",
      "epoch number: 500 loss: 45178.2 accuracy 15: 0.205609\n",
      "epoch number: 500 accuracy 10: 0.187373\n",
      "epoch number: 500 accuracy 5: 0.159666\n",
      "epoch number: 500 loss: 45148.2 accuracy 15: 0.203778\n",
      "epoch number: 500 accuracy 10: 0.185883\n",
      "epoch number: 500 accuracy 5: 0.158686\n",
      "epoch number: 500 loss: 45118.0 accuracy 15: 0.206411\n",
      "epoch number: 500 accuracy 10: 0.188425\n",
      "epoch number: 500 accuracy 5: 0.159825\n",
      "epoch number: 510 loss: 44280.3 accuracy 15: 0.206566\n",
      "epoch number: 510 accuracy 10: 0.188411\n",
      "epoch number: 510 accuracy 5: 0.159851\n",
      "epoch number: 510 loss: 44250.5 accuracy 15: 0.209975\n",
      "epoch number: 510 accuracy 10: 0.191303\n",
      "epoch number: 510 accuracy 5: 0.162367\n",
      "epoch number: 510 loss: 44220.8 accuracy 15: 0.201952\n",
      "epoch number: 510 accuracy 10: 0.183903\n",
      "epoch number: 510 accuracy 5: 0.156761\n",
      "epoch number: 520 loss: 43392.9 accuracy 15: 0.207172\n",
      "epoch number: 520 accuracy 10: 0.188793\n",
      "epoch number: 520 accuracy 5: 0.160138\n",
      "epoch number: 520 loss: 43363.6 accuracy 15: 0.206932\n",
      "epoch number: 520 accuracy 10: 0.188718\n",
      "epoch number: 520 accuracy 5: 0.160247\n",
      "epoch number: 520 loss: 43334.2 accuracy 15: 0.202742\n",
      "epoch number: 520 accuracy 10: 0.184576\n",
      "epoch number: 520 accuracy 5: 0.15679\n"
     ]
    }
   ],
   "source": [
    "with open('dis_xent.csv', 'wb') as csvfile:\n",
    "    wr = csv.writer(csvfile, delimiter='\\t', lineterminator='\\n')\n",
    "    for epoch in range(int(n_opt_epoch)):\n",
    "        if(epoch != 0):\n",
    "            writer_opt.add_summary(summary ,epoch)\n",
    "        for batch_n in range(int(n_users / batch_size)):\n",
    "            rw_to_chose = np.random.choice((tr_likes[\"user_id\"].unique()) ,batch_size ,replace = False)\n",
    "            x_tr = sort_pd_df_by_ext_vec(tr_likes\n",
    "                                         , rw_to_chose, cols = ['user_id','rank'])\n",
    "            x_tr_dis = sort_pd_df_by_ext_vec(train_dislike\n",
    "                                         , rw_to_chose, cols = ['user_id','rank'])\n",
    "            y_batch = sort_pd_df_by_ext_vec(tr_y ,rw_to_chose,cols = ['user_id','rank'])\n",
    "            length_vec = sort_pd_df_by_ext_vec(df = max_watch ,ext_sor_vec = rw_to_chose ,cols = ['user_id']) \n",
    "            length_max = max(length_vec.iloc[:,1])\n",
    "            str_idx = algeb_geom_series(0 ,start = 0 ,jump = length_max ,length = batch_size)\n",
    "            end_idx = np.append(length_vec.iloc[0, 1], length_vec.iloc[1:, 1:] + str_idx[1:].reshape(batch_size-1, -1))\n",
    "            app_range = range_bet_col_t_col_n_append(str_idx, end_idx)\n",
    "            trial_size = (int(batch_size * length_max), n_feature)\n",
    "            trial_size_dis = (int(batch_size * fixed), n_feature_dis)\n",
    "            zero_array_x = np.zeros(trial_size) \n",
    "            zero_array_x_dis = np.zeros(trial_size_dis)\n",
    "            _x_tr = np_pad_tr_x(x_tr.iloc[:,3:], batch_size, str_idx.astype(int),\n",
    "                                            zero_array_x, length_vec.iloc[:,1].astype(int))\n",
    "\n",
    "            rep_ind = np.concatenate((np.repeat(app_range, n_samples).reshape(-1, 1)\n",
    "                                      , np.repeat(y_batch['desired'], n_samples).reshape(-1, 1)), axis = 1)\n",
    "\n",
    "            (acc_r, acc_l ,acc_k, xen, _, summary) = sess.run([accuracy_r, accuracy_l\n",
    "                                                                                  , accuracy_k, xent_regu \n",
    "                                                                                  ,optimizer, summary_op]\n",
    "                                                      ,feed_dict = {dynam_input: _x_tr.reshape(batch_size, -1, n_feature)\n",
    "                                                      ,dislike_inp: np.array(x_tr_dis.iloc[:,3:]).reshape(batch_size, -1, n_feature_dis)\n",
    "                                                      ,tr_rw_n_desired_rep: rep_ind\n",
    "                                                      ,tr_rw : app_range.reshape(-1, 1) \n",
    "                                                      ,y_true: y_batch['desired'].reshape(-1, 1)\n",
    "                                                      ,max_batch_length: length_vec.iloc[:, 1]})\n",
    "            wr.writerow([acc_r])\n",
    "            wr.writerow([acc_l])\n",
    "            wr.writerow([acc_k])\n",
    "            #wr.writerow([tr_mrr])\n",
    "            if (epoch%10 == 0):\n",
    "                print('epoch number:', epoch, 'loss:', xen, 'accuracy 15:', acc_k)\n",
    "                print('epoch number:', epoch, 'accuracy 10:', acc_l)\n",
    "                print('epoch number:', epoch, 'accuracy 5:', acc_r)\n",
    "\n",
    "\n",
    "        folder = './reco_rnn/dis_xent/'\n",
    "        save_path = saver.save(sess, folder + 'accu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20332171"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rw_to_chose = np.random.choice(range(int(tr_users), n_users)\n",
    "                               , n_users - int(tr_users), replace = False)\n",
    "x_tr = sort_pd_df_by_ext_vec(user_vid_time_vidfeat_usefit_sorted\n",
    "                             , rw_to_chose, cols = ['user_id','time_watch'])\n",
    "x_tr_dis = sort_pd_df_by_ext_vec(user_vid_time_vidfeat_usefit_sorted_dis\n",
    "                             , rw_to_chose, cols = ['user_id','time_watch'])\n",
    "y_batch = sort_pd_df_by_ext_vec(te_y ,rw_to_chose,cols = ['user','rank'])\n",
    "length_vec = sort_pd_df_by_ext_vec(df = max_watch ,ext_sor_vec = rw_to_chose ,cols = ['user']) \n",
    "length_max = max(length_vec.iloc[:,1])\n",
    "str_idx = algeb_geom_series(0 ,start = 0 ,jump = length_max ,length = batch_size)\n",
    "end_idx = np.append(length_vec.iloc[0, 1], length_vec.iloc[1:, 1:] + str_idx[1:].reshape(batch_size-1, -1))\n",
    "app_range = range_bet_col_t_col_n_append(str_idx, end_idx)\n",
    "trial_size = (int(batch_size * length_max), n_feature)\n",
    "zero_array_x = np.zeros(trial_size) ; zero_array_x_dis = np.zeros(trial_size)\n",
    "_x_tr = np_pad_tr_x(x_tr.iloc[:,3:], batch_size, str_idx.astype(int),\n",
    "                                zero_array_x, length_vec.iloc[:,1].astype(int))\n",
    "_x_tr_dis = np_pad_tr_x(x_tr_dis.iloc[:,3:], batch_size, str_idx.astype(int),\n",
    "                                zero_array_x_dis, length_vec.iloc[:,1].astype(int))  \n",
    "rep_ind = np.concatenate((np.repeat(app_range, n_samples).reshape(-1, 1)\n",
    "                          , np.repeat(y_batch['desired'], n_samples).reshape(-1, 1)), axis = 1)\n",
    "non_rep = np.concatenate((app_range).reshape(-1, 1)\n",
    "                          , (y_batch['desired']).reshape(-1, 1), axis = 1)\n",
    "correct_inco, lik_fac, rel_lik, dis, lik, test_accuracy_r, test_accuracy_l ,test_accuracy_k, test_mrr, test_user_acc, \n",
    "test_loss  = sess.run([to_bool_k, like_factor, _relative_like, _dislike_score, _like_score, accuracy_r, accuracy_l , accuracy_k, mrr, to_bool_user, p_wise]\n",
    "                                          ,feed_dict = {dynam_input: _x_tr.reshape(batch_size, -1, n_feature)\n",
    "                                          ,dislike_inp: _x_tr_dis.reshape(batch_size, -1, n_feature)\n",
    "                                          ,tr_rw_n_desired_rep: rep_ind\n",
    "                                          ,tr_rw_non_rep: non_rep  \n",
    "                                          ,tr_rw : app_range.reshape(-1, 1) \n",
    "                                          ,y_true: y_batch['desired'].reshape(-1, 1)\n",
    "                                          ,max_batch_length: length_vec.iloc[:, 1]})\n",
    "\n",
    "print('accuracy top 5:', test_accuracy_r) \n",
    "print('accuracy top 10:', test_accuracy_l) \n",
    "print('accuracy top 15:', test_accuracy_k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "for user in range(n_users): \n",
    "    user_like_fac = lik_fac[str_idx[user] : end_idx[user]].reshape(-1, 1)\n",
    "    user_cor_inco = correct_inco[str_idx[user] : end_idx[user]].reshape(-1, 1)\n",
    "    data{user} = user_like_fac * user_cor_inco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "heatmap = plt.pcolor(data{}, cmap=matplotlib.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>End session</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
