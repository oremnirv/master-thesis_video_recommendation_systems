{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><u>RNN thesis</u></h1>\n",
    "<h4>Omer Nivron</h4>\n",
    "15098443"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Package loading</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys as sys\n",
    "import random as rd\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import pdb\n",
    "from tensorflow.python.ops import tensor_array_ops, control_flow_ops\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import boto3\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import cStringIO\n",
    "#! sudo pip install s3fs\n",
    "import s3fs\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def np_pad_pos_sample(pos, batch_size, str_idx, zero_array_x, length_vec, num_samp):\n",
    "    start = 0\n",
    "    for i in range(batch_size):\n",
    "        if i > 0:\n",
    "            end = end + int(length_vec[i]*num_samp) \n",
    "        else:\n",
    "            end = length_vec[0]*num_samp\n",
    "        srt_ix = int(str_idx[i]*num_samp) \n",
    "        end_ix = (int(str_idx[i]*num_samp) + int(length_vec[i]*num_samp))\n",
    "        \n",
    "        zero_array_x[ srt_ix : end_ix] = pos[ start : end ] \n",
    "        start = end\n",
    "    return(zero_array_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def np_pad_tr_x(x_tr, batch_size, str_idx, zero_array_x, length_vec):\n",
    "    start = 0\n",
    "    for i in range(batch_size):\n",
    "        if i > 0:\n",
    "            end = end + length_vec[i] \n",
    "        else:\n",
    "            end = length_vec[0]\n",
    "        zero_array_x[ str_idx[i] : (str_idx[i] + length_vec[i]) ] = x_tr[ start : end ] \n",
    "        start = end\n",
    "\n",
    "    return(zero_array_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_bet_col_t_col_n_append(col_1 ,col_2):\n",
    "    app_ranges=[]\n",
    "    for i in range(col_1.shape[0]):\n",
    "        single_range = range((col_1[i]).astype(int) ,(col_2[i]).astype(int))\n",
    "        app_ranges = np.append(app_ranges,single_range)\n",
    "    return(app_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algeb_geom_series(mode ,start ,jump ,length):\n",
    "    u = np.empty((length,))\n",
    "    u[0] = start\n",
    "    u[1:] = jump\n",
    "    if (mode == 0):\n",
    "        series=np.cumsum(u)\n",
    "    if (mode == 1):\n",
    "        series=np.cumprod(u)\n",
    "    return(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_pd_df_by_ext_vec(df,ext_sor_vec,cols):\n",
    "    df_s = df[((df[cols[0]]).astype(int)).isin(ext_sor_vec)] #\n",
    "    df_s['sort_cat'] = pd.Categorical(df_s[cols[0]],categories = ext_sor_vec,ordered = True)\n",
    "    if len(cols) > 1:\n",
    "        df_s.sort_values(['sort_cat',cols[1]] ,inplace = True)\n",
    "    \n",
    "    else:\n",
    "        df_s.sort_values(['sort_cat'],inplace = True) \n",
    "    \n",
    "    df_s.reset_index(inplace = True)\n",
    "    df_ = df_s.drop(['sort_cat','index'] ,axis = 1)\n",
    "    \n",
    "    return(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_pd_df_by_ext_vec_rec_rank(df,ext_sor_vec,cols):\n",
    "    df_s = df[((df[cols[0]]).astype(int)).isin(ext_sor_vec)] #\n",
    "    df_s['sort_cat'] = pd.Categorical(df_s[cols[0]],categories = ext_sor_vec,ordered = True)\n",
    "    if len(cols) > 1:\n",
    "        df_s.sort_values(['sort_cat',cols[1],cols[2]] ,inplace = True)\n",
    "    \n",
    "    else:\n",
    "        df_s.sort_values(['sort_cat'],inplace = True) \n",
    "    \n",
    "    df_s.reset_index(inplace = True)\n",
    "    df_ = df_s.drop(['sort_cat','index'] ,axis = 1)\n",
    "    \n",
    "    return(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainSamples(viewers,videos,probab,viewerFeat,videoFeat,contxFeat):\n",
    "    trData = {} # trData = Dictionary with training data. This is histrory of viewer and video iteraction\n",
    "    X = {} # X  = Dictionary with viewer features as arrays\n",
    "    Y ={} # Y  = Dictionary with video features as arrays\n",
    "    for i in range(viewers):\n",
    "        X[i] = np.random.rand(viewerFeat)\n",
    "        a = 0 # timing of the video for a particular user,... \n",
    "                #to give the order in which the videos have been watched\n",
    "        for j in range(videos):\n",
    "            if int(np.random.binomial(1,probab ,1)[0]):\n",
    "                trData[(i,j,a)] = np.random.rand(contxFeat)\n",
    "                a+=1 # when a video is watched, we increase the value of a by 1 \n",
    "            if i==0:\n",
    "                Y[j] = np.random.rand(videoFeat)\n",
    "    return X,Y,trData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_model(model ,n_hidden ,layers):\n",
    "    if (layers == 1):\n",
    "        if(model == 'lstm'):\n",
    "            cell = tf.nn.rnn_cell.LSTMCell(n_hidden ,state_is_tuple=True)\n",
    "        else:\n",
    "            cell = tf.nn.rnn_cell.GRUCell(n_hidden)   \n",
    "    else:\n",
    "        if(model == 'lstm'):\n",
    "            lstm = tf.nn.rnn_cell.LSTMCell(n_hidden ,state_is_tuple=True)\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell([lstm]*layers)\n",
    "\n",
    "        else:\n",
    "            gru = tf.nn.rnn_cell.GRUCell(n_hidden)\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell([gru] * layers)\n",
    "    return(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewers = 500  #number of viewers\n",
    "videos =500  #number of videos\n",
    "probab = 0.1  #probability of a viewer watching any one video\n",
    "viewerFeat = 310  #number of features describing a veiwer\n",
    "videoFeat = 300   #number of features describing a video\n",
    "contxFeat = 15 # number of contextual features\n",
    "# X  = Dictionary with viewer features as arrays\n",
    "# Y  = Dictionary with video features as arrays\n",
    "# trData = Dictionary with training data. This is histrory of viewer and video iteraction\n",
    "X ,Y ,trData = trainSamples(viewers ,videos ,probab ,viewerFeat ,videoFeat ,contxFeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userFiltNum = 1000\n",
    "user_feat_inp_w_key_df = user_feat_inp_w_key_df2[user_feat_inp_w_key_df2['user_id'] < userFiltNum]\n",
    "user_vid_time_df  = user_vid_time_df2[user_vid_time_df2['user_id'] < userFiltNum]\n",
    "num_user_feat, num_video_feat = user_feat_inp_w_key_df.shape[1] -1 , vid_feat_inp_w_key_df.shape[1] -1\n",
    "num_users, contex_feat = user_feat_inp_w_key_df.shape[0], len(user_vid_time_df.columns[3:])\n",
    "\n",
    "user_vid_time_df_sort=user_vid_time_df[[\"user_id\",'movie_id',\"rank\"]].sort_values([\"user_id\",\"rank\"])\n",
    "user_vid_time_df_sort = user_vid_time_df_sort.reset_index(drop=True)\n",
    "\n",
    "h=pd.get_dummies(user_vid_time_df_sort[\"movie_id\"],sparse=True,prefix='vid_')\n",
    "\n",
    "y_tr_p_w=pd.concat([user_vid_time_df_sort.reset_index(drop=True), h], axis=1)\n",
    "y_tr_p_w['desired'] = np.argmax(np.array(y_tr_p_w.iloc[:,3:]) ,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform a dict to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feat_inp = np.array([X[key] for key in sorted(X.keys())]) \n",
    "vid_feat_inp = np.array([Y[key] for key in sorted(Y.keys())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a column key specifying user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_user = np.asarray(range(user_feat_inp.shape[0])).reshape(user_feat_inp.shape[0] ,1)\n",
    "key_vid = np.asarray(range(vid_feat_inp.shape[0])).reshape(vid_feat_inp.shape[0] ,1)\n",
    "user_feat_inp_w_key = np.concatenate((user_feat_inp ,key_user),axis = 1)\n",
    "vid_feat_inp_w_key = np.concatenate((vid_feat_inp ,key_vid),axis = 1)\n",
    "user_vid_time = trData.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform numpy to pandas df in order to use easy merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_feat_inp_w_key_df = pd.DataFrame(user_feat_inp_w_key) \n",
    "vid_feat_inp_w_key_df = pd.DataFrame(vid_feat_inp_w_key)\n",
    "user_vid_time_df = pd.DataFrame(user_vid_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort values by user and time watched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rr = user_vid_time_df.sort_values([0 ,2]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get categorical column for each video with 1 where watched and 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h = pd.get_dummies(rr[1],prefix = 'vid_')\n",
    "y_tr_p_w = pd.concat([rr.reset_index(drop = True), h], axis = 1)\n",
    "y_tr_p_w.rename(columns = {0: 'user'}, inplace = True)\n",
    "y_tr_p_w.rename(columns = {1: 'mov'}, inplace = True)\n",
    "y_tr_p_w.rename(columns = {2: 'rank'}, inplace = True)\n",
    "y_tr_p_w['desired'] = np.argmax(np.array(y_tr_p_w.iloc[:,4:]) ,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_desire = y_tr_p_w.iloc[:,[1,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (users_desire.groupby('user').agg(lambda x: x.tolist())).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_wind(lst, slide_size, user):\n",
    "    result_ = []\n",
    "    for i in range(0, len(lst) - slide_size + 1):\n",
    "        result_.extend(lst[i:i + slide_size])\n",
    "    for j in range(len(lst)-slide_size + 1, len(lst)):\n",
    "        result_.extend(np.concatenate((lst[j:len(lst)], lst[0:slide_size - (len(lst)-j)]), 0))\n",
    "    vid_id = np.reshape(result_, [-1, 1])\n",
    "    row = np.repeat(range(int(len(result_) / slide_size)), slide_size).reshape(-1 ,1)\n",
    "    user_x = np.repeat(user, np.shape(result_)[0]).reshape(-1,1)\n",
    "    row_n_user = np.array(np.concatenate((user_x, row), axis = 1)).reshape(-1,2)\n",
    "    row_n_user_n_vid = np.concatenate((row_n_user, vid_id), axis=1)\n",
    "    desired_rank = np.tile(range(1, slide_size + 1), int(len(result_) / slide_size)).reshape(-1, 1)\n",
    "    row_n_user_n_vid_n_rank = np.concatenate((desired_rank, row_n_user_n_vid), axis=1)\n",
    "    return row_n_user_n_vid_n_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = t['user'].apply(lambda row: sliding_wind(t['desired'][row], 30, row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array = pd.DataFrame(np.vstack(z))\n",
    "new_array.rename(columns = {0: 'rank'}, inplace = True)\n",
    "new_array.rename(columns = {1: 'user_id'}, inplace = True)\n",
    "new_array.rename(columns = {2: 'time'}, inplace = True)\n",
    "new_array.rename(columns = {3: 'movie_id'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_watch = (y_tr_p_w.groupby('user',axis = 0).sum().iloc[:\n",
    "                                ,int(np.array(np.where(y_tr_p_w.columns=='rank'))):-1].sum(axis=1)).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge two pandas df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vid_feat_inp_w_key_df.rename(columns = {300: 'mov_id'}, inplace = True)\n",
    "user_feat_inp_w_key_df.rename(columns = {310: 'user_id'}, inplace = True)\n",
    "user_vid_time_df.rename(columns = {0: 'user_id'}, inplace = True)\n",
    "user_vid_time_df.rename(columns = {1: 'mov_id'}, inplace = True)\n",
    "user_vid_time_vidfeat = user_vid_time_df.merge(vid_feat_inp_w_key_df \n",
    "                                                             ,how = 'inner' ,on='mov_id' ,sort = False)\n",
    "user_vid_time_vidfeat.rename(columns={'2_x': 'time_watch'} ,inplace=True)\n",
    "user_vid_time_vidfeat_sorted = user_vid_time_vidfeat.sort_values(['user_id', 'time_watch']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_users = viewers * 0.8\n",
    "tr_y = y_tr_p_w[y_tr_p_w['user'] < tr_users]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Configurations</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LSTM with X units</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_opt_epoch = 1\n",
    "model = 'lstm'\n",
    "slide_size = 10\n",
    "layers = 1\n",
    "top_k = 5\n",
    "n_samples = 1000\n",
    "n_users = 129\n",
    "te_users = int(n_users * 0.2)\n",
    "n_feature = 300\n",
    "n_user_feature = 310\n",
    "lr_rat = 0.001\n",
    "num_video = 6000\n",
    "beta = 0.01\n",
    "if tr_users > 128:\n",
    "    batch_size = 128\n",
    "else:\n",
    "    batch_size = int(tr_users)\n",
    "n_hidden = 32 # hidden layer num of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard saving paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logs_path = \"/Users/onivron/Desktop/reco_rnn/tensor_plot/recip/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Var creation</h3>\n",
    "We build data placeholders & variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('input'):\n",
    "    dynam_input = tf.placeholder(\"float32\" ,[batch_size ,None ,n_feature] ,name = 'dynam_input') \n",
    "    const_input = tf.placeholder(\"float32\" ,[batch_size ,n_user_feature] ,name = 'const_input') \n",
    "    #tr_rw = tf.placeholder(\"int32\" ,[None,1] ,name = '_rw')\n",
    "    #tr_rw_n_desired_rep = tf.placeholder(\"int32\" ,[None, 2] ,name = 'rw_rep')\n",
    "    y_true = tf.placeholder(\"int32\",[None, 1] ,name = 'Input_y')\n",
    "    rank_inp = tf.placeholder(\"int32\",[None, 4] ,name = 'Input_y')\n",
    "    max_batch_length = tf.placeholder(\"float32\" ,[batch_size] ,name = 'max_leng')\n",
    "    \n",
    "r_lo = tensor_array_ops.TensorArray(dtype = tf.float32, size = slide_size,\n",
    "                                    dynamic_size = False, infer_shape = True)\n",
    "\n",
    "with tf.name_scope(\"weights\"):\n",
    "    W = {'const': tf.Variable(tf.random_normal([n_user_feature ,num_video]), name = 'const_1'),\n",
    "        'out_2': tf.Variable(tf.random_normal([n_hidden ,num_video]), name = 'w_2')}\n",
    "    \n",
    "with tf.name_scope(\"biases\"):\n",
    "    b = {'const': tf.Variable(tf.random_normal([num_video]), name = 'b_1'),\n",
    "        'out_2': tf.Variable(tf.random_normal([num_video]), name = 'b_3')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(dynam_input ,const_input \n",
    "        ,max_batch_length\n",
    "         ,W ,b ,model):\n",
    "    \n",
    "    lstm_cell = rnn_model(model ,n_hidden ,layers)\n",
    "    \n",
    "    outputs ,states = tf.nn.dynamic_rnn(lstm_cell ,inputs = dynam_input\n",
    "                                     ,dtype = tf.float32 ,sequence_length = max_batch_length)\n",
    "    \n",
    "    out_shaped = tf.reshape(outputs ,[-1 ,n_hidden])\n",
    "    lay_2 = tf.reshape(tf.reshape(tf.matmul(out_shaped ,W['out_2']) + b['out_2']\n",
    "                       ,[-1,batch_size,num_video]) + (tf.matmul(const_input,\n",
    "                                                                W['const']) + b['const']), [-1, num_video])\n",
    "    lay_3 = tf.nn.softmax(lay_2)\n",
    " \n",
    "    return (lay_2, lay_3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank_loss(i, rank_inp, y_pred, y_softmax, r_lo, slide_size):\n",
    "    _slide_size = slide_size\n",
    "    _rank_inp = rank_inp\n",
    "    _y_pred = y_pred\n",
    "    _y_softmax = y_softmax\n",
    "    tar_idx = tf.to_int32(tf.where(tf.equal(_rank_inp[:, 2], i))[:, 0])\n",
    "    _col = tf.reshape(tf.tile([2, 3], [ tf.size(tar_idx)]), [-1, 2])  \n",
    "    _target_ind = tf.concat((tf.reshape(tar_idx, [-1, 1]), _col), axis = 1)\n",
    "    target_idx = tf.gather(_rank_inp, indices = _target_ind)\n",
    "    target = tf.reshape(tf.gather(params = tf.reshape(_y_pred\n",
    "                                                        , [-1, num_video]), indices = target_idx), [-1, 1])\n",
    "    _target = tf.tile(target, [1, tf.size(slide_size) - i ])  \n",
    "    _target = tf.reshape(_target, [-1]) \n",
    "    com_idx = (tf.to_int32(tf.where(tf.not_equal(_rank_inp[:, 2], i))))[:, 0]\n",
    "    _col_com = tf.reshape(tf.tile([2, 3], [tf.size(com_idx)]), [-1, 2])  \n",
    "    to_compare_idx =  tf.concat((tf.reshape(com_idx, [-1, 1]), _col_com), axis = 1)\n",
    "    compare_idx = tf.gather(_rank_inp, indices = to_compare_idx)\n",
    "    to_compare_weight = tf.reshape(tf.gather(params = tf.reshape(_y_softmax\n",
    "                                                    , [-1, num_video]), indices = compare_idx), [-1, 1])\n",
    "    to_compare = tf.reshape(tf.gather(params = tf.reshape(_y_pred\n",
    "                                                    , [-1, num_video]), indices = compare_idx), [-1, 1])\n",
    "    lo = tf.reduce_mean(-tf.log(tf.reduce_sum(tf.multiply(tf.sigmoid(tf.subtract(_target\n",
    "                                                            , tf.reshape(to_compare, [-1])))\n",
    "                                                            ,to_compare_weight))))\n",
    "    r_lo = r_lo.write(i, lo)\n",
    "    \n",
    "    return i+1, _rank_inp, _y_pred, _y_softmax , r_lo, _slide_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define loss, accuracy & optimizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_softmax = RNN(dynam_input, const_input ,max_batch_length, W, b, model)\n",
    "\n",
    "\n",
    "\n",
    "_, _, _, _, _ran_loss, _ = tf.while_loop(\n",
    "        cond=lambda i, _1, _2, _3, _4, _5: i < slide_size,\n",
    "        body = rank_loss,\n",
    "        loop_vars=(tf.constant(1, dtype=tf.int32),\n",
    "                   rank_inp,\n",
    "                   y_pred, y_softmax, r_lo, slide_size))\n",
    "#mean_ran_loss = tf.reduce_mean(_ran_loss[:,1])\n",
    "\n",
    "\n",
    "#with tf.name_scope('mean_reciprocal'):\n",
    "#    target_ind = tf.gather_nd(params = tf.reshape(y_pred, [-1, num_video]), indices = tr_rw_n_desired_rep)\n",
    "#    relevant_y_pred = tf.reshape(tf.gather(params = tf.reshape(y_pred\n",
    "#                                                    ,[-1 ,num_video]), indices = tr_rw), [-1, num_video])\n",
    "#    top_neg_vals, top_neg_indice = tf.nn.top_k(relevant_y_pred, n_samples)\n",
    "#    softmax_scores = tf.reshape(tf.nn.softmax(top_neg_vals), [-1])\n",
    "#    loss = tf.reduce_mean(-tf.log(tf.reduce_sum(tf.multiply(tf.sigmoid(tf.subtract(target_ind\n",
    "#                                                            , tf.reshape(top_neg_vals, [-1])))\n",
    "#                                                            ,softmax_scores))))\n",
    "#    regularizer = tf.nn.l2_loss(tf.abs(W['out_2']))\n",
    "#    mean_reciprocal = tf.reduce_mean(loss + mean_ran_loss  + beta * regularizer)\n",
    "\n",
    "#with tf.name_scope('Accuracy'):\n",
    "#    top_15_indx = tf.slice(top_neg_indice, [0, 0], [-1, top_k])\n",
    "#    to_bool = tf.reduce_sum(tf.cast(tf.equal(y_true, top_15_indx), tf.float32), 1)\n",
    "#    accuracy = tf.reduce_mean(to_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with tf.name_scope('train'):    \n",
    "#    optimizer = tf.train.AdamOptimizer(learning_rate = lr_rat).minimize(mean_reciprocal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tensorboard set-up</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "tf.summary.scalar(\"cost\" ,p_wise)\n",
    "tf.summary.scalar(\"accuracy\" ,accuracy)\n",
    "summary_op = tf.summary.merge_all()\n",
    "writer_opt = tf.summary.FileWriter(logs_path ,graph = tf.get_default_graph())\n",
    "saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training - optimization</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session() \n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading previous parameters and continuing training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "reload_tf_weights('/Users/onivron/Desktop/reco_rnn/pair_cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onivron/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/Users/onivron/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/onivron/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/onivron/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:27: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/Users/onivron/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:37: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    }
   ],
   "source": [
    "with open('rec_rank_accuracy.csv', 'wb') as csvfile:\n",
    "    wr = csv.writer(csvfile, delimiter='\\t', lineterminator='\\n')\n",
    "    for epoch in range(int(n_opt_epoch)):\n",
    "        if(epoch != 0):\n",
    "            writer_opt.add_summary(summary ,epoch)\n",
    "        for batch_n in range(int(n_users / batch_size)):\n",
    "\n",
    "            rw_to_chose = np.random.choice(int(tr_users) ,batch_size ,replace = False)\n",
    "            pos_ex = sort_pd_df_by_ext_vec(new_array, rw_to_chose,\n",
    "                                              cols = ['user_id','time'])\n",
    "\n",
    "            const_tr = (sort_pd_df_by_ext_vec(user_feat_inp_w_key_df, rw_to_chose,\n",
    "                                              cols = ['user_id'])).drop(['user_id'], axis = 1)\n",
    "            x_tr = sort_pd_df_by_ext_vec(user_vid_time_vidfeat_sorted,\n",
    "                                          rw_to_chose, cols = ['user_id','time_watch'])\n",
    "            y_batch = sort_pd_df_by_ext_vec(tr_y, rw_to_chose,cols = ['user','rank'])\n",
    "\n",
    "            length_vec = sort_pd_df_by_ext_vec(df = max_watch ,ext_sor_vec = rw_to_chose ,cols = ['user'])      \n",
    "            length_max = max(length_vec.iloc[:,1])\n",
    "            str_idx = algeb_geom_series(0 ,start = 0 ,jump = length_max ,length = batch_size)\n",
    "            end_idx = np.append(length_vec.iloc[0, 1], length_vec.iloc[1:, 1:] + str_idx[1:].reshape(batch_size-1, -1))\n",
    "            app_range = range_bet_col_t_col_n_append(str_idx, end_idx)\n",
    "            pos_ex['time'] = pos_ex['time'] + np.repeat(str_idx, length_vec.iloc[:,1] * slide_size) \n",
    "            trial_size = (int(batch_size * length_max), n_feature)\n",
    "            zero_array_x = np.zeros(trial_size)\n",
    "            _x_tr = np_pad_tr_x(x_tr.iloc[:,3:], batch_size, str_idx.astype(int),\n",
    "                                            zero_array_x, length_vec.iloc[:,1].astype(int))       \n",
    "            rep_ind = np.concatenate((np.repeat(app_range, n_samples).reshape(-1, 1)\n",
    "                                      , np.repeat(y_batch['desired'], n_samples).reshape(-1, 1)), axis = 1)\n",
    "\n",
    "\n",
    "            #acc,p_wise_cost ,_ ,summary\n",
    "            loss_trial = sess.run([(_ran_loss.stack())]\n",
    "                    #,accuracy, p_wise, optimizer, summary_op]\n",
    "                                                      ,feed_dict = {dynam_input: _x_tr.reshape(batch_size,-1,n_feature)\n",
    "                                                      ,const_input: const_tr \n",
    "                                                      #,tr_rw_n_desired_rep: rep_ind\n",
    "                                                      #,tr_rw: app_range.reshape(-1, 1)\n",
    "                                                      ,y_true: y_batch['desired'].reshape(-1, 1)\n",
    "                                                      ,rank_inp: np.array(pos_ex)\n",
    "                                                      ,max_batch_length: length_vec.iloc[:,1]})\n",
    "        wr.writerow([acc])    \n",
    "\n",
    "\n",
    "        folder = '/Users/onivron/Desktop/reco_rnn/reciprocal/'\n",
    "        save_path = saver.save(sess ,folder + 'accu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sess.run([(tf.to_int32(tf.where(tf.equal( ([[7,7,7,7,7,7,2, 2], [7,7,7,7,7,7,3,2]]), [2]))))[:, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>End session</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
